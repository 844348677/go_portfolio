1. Introducing Kubernetes

monoliths
legacy system
slow release cycles and are updated relatively infrequently
developers package up the whole system and hand it over to the ops teams

microservices
microservices are decoupled from each other, they can be developed, deployed , updated , and scaled individually
But with bigger numbers of deployable components and increasingly larger datacenters, it becomes increasingly difficult to configure, manage, and keep the wholesystem running smoothly.
resource utilization and hardware costs
automation

abstracts away the hardware infrastructure
deploy and run 
When deploying a multi-component application through Kubernetes, it selects a server for each component, deploys it, and enables it to easily find and communicate with all the other components of your application.

largest datacenters , such as the ones built and operated by cloud providers

Changes
a consequence of splitting big monolithic apps into smaller microservices
and the change in the infrastructure that runs those apps

Moving from monolithc apps to microservices
develop, deployed , and managed as one entity . run as a single OS process
Changes to one part fo the application require a redeployment of the whole application
certain parts of an application are extremely hard or next to impossible to scale horizontally

Splitting apps into microservices
each microservices run as an independent process and communicates with other microservices through simple, well-define interfaces
API

Scaling microservices
a per-serivce basis
which means you have the option of scaling only those services  that require more resources, while leaving others at their original scale
splitting the app into microservices allow you to horizontally scale the parts that allow scaling out

Deploying microservices
microservices also have drawbacks
components increases
making it hard to debug and trace execution calls
distributed tracing system such as Zipkin

Understanding the divergence of environment requirements
the bigger the number of components you need to deploy on the same host, the harder it will be to manage all  their dependencies to satisfy all their requirement

Providing a consistent environment ot applications
huge difference between development and production environment, difference even exist between individual production machines
developers often take care of their development laptops on their own

Moving to continuous delivery : DevOps and NoOps
the same team that develops the application also take part in deploying it and taking care of it over whole lifetime
this means the developer, QA, and operations teams now need to collaborate throughtout the whole process. this practice is called DevOps

Understanding the benefits
release newer versions of application nore often

Letting developers and sysadmins do what they do best
Kubernetes enables us to achieve all of this. By abstracting away the actual hardware and exposing it as a single platform for deploying and running apps, it allows developers to configure and deploy their applications without any help from the sysadmins and allows the sysadmins to focus on keeping the underlying infrastructure up and running, while not having to know anything about the actual applications running on top of it.

Introducing container technologies
kubernetes uses Linux container technologies to provide isolate of runing application, 
Kubernetes, Docker,  rkt

Understanding what containers are
only smaller numbers of large components. Virtual Machine (VM)
But when these components start getting smaller and their numbers start to grow

Isolating components with Linux container technologies
Instead of using virtual machines to isolate the environments of each microservice (or software processes in general), developers are turning to Linux container technologies.
run multiple services on the same host machine
not only exposing a different environment to each of them, but also isolating them from each other
A process running in a container runs inside the host’s operating system, like all the other processes
VMs, where processes run in separate operating systems
But the process in the container is still isolated from other processes.

Comparing virtual machine to containers
coantainers are much more lightweight
nothing more than a single isolated process running in the host OS, consuming only the resources that the app consumes and without the overhead of any additional processes
you often end up grouping multiple applications into each VM because you don’t have enough resources to dedicate a whole VM to each app. 
When using containers, you can (and should) have one container for each application

the main benefit of virtual machines is the full isolation they provide, because each VM runs its own Linux kernel,
while containers all call out to the same kernel, which can clearly pose a security risk

introducing the mechanisms that make container isolation possible
the first one ,Linux Namespaces, makes sure each process see its own personal view of the system (file,process, network interfaces,hostname)
the second on is Linux Control Groups (cgroups) , which limit the amount of resources the process can consume(CPU,memory,network bandwith)

Isolating processes with Linux Namespaces
by default, each Linux system initially has one single namespace.
All system resources, such as filesystems, process IDs, user IDs, network interfaces, and others, belong to the single namespace.
But you can create additional namespaces and organize resources across them.
When running a process, you run it inside one of those namespaces. The process will only see resources that are inside the same namespace.
a process doesn’t belong to one namespace, but to one namespace of each kind.
namespace of each kind.
The following kinds of namespaces exist:
	Mount (mnt)
	Process ID (pid)
	Network (net)
	Inter-process communication (ipc)
	UTS
	User ID (user)
each namespace kind is used to isolate a certain group of resources
By assigning two different UTS namespaces to a pair of processes, you can make them see different local hostnames.

Limiting resource available to a process
cgroups, a Linux kernel feature that limits the resource usage of a process(or a group of processes)
a processes can't use more than the configured amount of CPU,memory,network bandwidth
This way, processes cannot hog resources reserved for other processes, which is similar to when each process runs on a separate machine.

Introducing the Docker container platform
It simplified the process of packaging up not only the application but also all its libraries and other dependencies, even the whole OS file system, into a simple, portable package that can be used to provision the application to any other machine running Docker.
a big different between Docker-based container images and VM images is that container images are composed of layers, which can be shared and reused across multiple images
This means only certain layers of an image need to be downloaded if the other layers were already downloaded previously when running a different container image that also contains the same layers

Understanding Docker concepts
Docker is a platform for packaging,distributing,and running applications
three main concepts in Docker comprise this scenario
Images : 
A Docker-based container image is something you package your application and its environment into.
it contains the filesystem that will be available to the application and other metadata, such as the path to the executable that should be executed when the image is run
Register :
A Docker Registry is a repository that stores your Dokcer images and facilitates easy sharing of the images between different people and computers
Containers :
A Docker-based container is a regular Linux container created from a Docker-based container image.
A running container is a process running on the host running Docker, but it’s completely isolated from both the host and all other processes running on it.

Building, Distributing, and Running a Docker image
1. developer tells Docker to build and push image
2. Docker builds image
3. Docker pushes image to registry
4. developer tells Docker on production machine to run image
5. Docker pulls image from registry
6. Docker runs container from image

Comparing virtual machine and Dokcer contianers
each container has its own isolated filesystem . how can both them share the same files?

Understanding image Layers
Docker images are composed of layers
different images can contain the exact same layer because every Docker image is built on top of another image and two different images can both use the parent image as their base
But layers don’t only make distribution more efficient, they also help reduce the storage footprint of images. Each layer is only stored once.
Two containers created from two images based on the same base layers can therefore read the same files, but if one of them writes over those files, the other one doesn’t see those changes.
Therefore, even if they share files, they’re still isolated from each other. This works because container image layers are read-only.
When a container is run, a new writable layer is created on top of the layers in the image. When the process in the container writes to a file located in one of the underlying layers, a copy of the whole file is created in the top-most layer and the process writes to the copy.

Understanding the portability limitations of container images
one small caveat exists : all containers running on a host use the host's Linux kernel
the different version of the Linux kernel
the different hardware architecture (x86,ARM-based)

Intoducing rkt - an alternative to Docker

Introducing Kubernetes
as the number of deployable application components in your system grows, it becomes harder to manage them all.
This has forced them to develop solutions for making the development and deployment of thousands of software components manageable and cost-efficient.

Understanding its origins
help both application developers and system administrators manage those thousands of applications and services.
After having kept Borg and Omega secret for a whole decade, in 2014 Google introduced Kubernetes, an open-source system based on the experience gained through Borg, Omega, and other internal Google systems.

Looking at Kubernetes from the top of a mountain
Kubernetes is a software system that allows you to easily deploy and manage containerized applications on top of it.
without having to know any internal details of these applications and without having to manually deploy these applications on each host.
This is of paramount importance for cloud providers, because they strive for the best possible utilization of their hardware while still having to maintain complete isolation of hosted applications.
Kubernetes enables you to run your software applications on thousands of computer nodes as if all those nodes were a single, enormous computer.
It abstracts away the underlying infrastructure and, by doing so, simplifies development, deployment, and management for both development and the operations teams.
Deploying applications through Kubernetes is always the same, whether your cluster contains only a couple of nodes or thousands of them.
Additional cluster nodes simply represent an additional amount of resources available to deployed apps.

Understanding the core of what kubernetes does
developer -> App descriptor -> Kubernetes master -> Tens or thousands of worker nodes exposed as a single deployment platform
a master node and any number of worker nodes
when the developer submits a list of apps to the master, kubernetes deploys them to the cluster of worke nodes. What node a component lands on doesn't (and shouldn't) matter
the developer can specify that certain apps must run together and Kubernetes will deploy them on the same worker node. Other will be spread around the cluster, but they can talk to each other in the same way, regardless of where they're deployed

Helping developers focus on the core app feature
Kubernetes can be thought of as an operating system for the cluster.
infrastructure-related services
Kubernetes to provide these services.
this includes things such as service discovery, scaling, load-balancing, self-healing, and even leader election

Helping OPS teams achieve better resource utilization
Kubernetes will run your containerized app somewhere in the cluster, provide information to its components on how to find each other, and keep all of them running.
Because your application doesn’t care which node it’s running on, Kubernetes can relocate the app at any time, and by mixing and matching apps, achieve far better resource utilization than is possible with manual scheduling.

Understanding the architecture of a Kubernetes cluster
at the hardware level, a kubernetes cluster is composed of many nodes, which can be split into two types
	the master node, which hosts the kubernetes Control Plane that controls and manages the whole Kubernetes system
	Worker nodes that run the actual application you deploy
Control Plane (master) :
etcd <- API server ; API server <- Schedule ; API server <- Controller Manager
Worker node(s) :
(master)API server <- Kubelet -> Container Runtime ; (master)API server <- kube-proxy

The Control Plane
The Control Plane is what controls the cluster and makes it functions.
in consist of multiple components that can run on a single master node or be split across multiple nodes and replicated to ensure high availability. these components are
	The Kubernetes API server, which you and the other Control Plane componenets communicate with
	The Schedule, which schedules your apps (assigns a worker node to each deployable component of your application)
	The Control Manager, which performs cluster-level functions, such as replicating components, keeping track of worker nodes, handling node failures , and so on
	etcd, a reliable distributed data store that persistently stores the cluster configuration
the components of the Control Plane hold and control the state of the cluster, but they don't run your applications. 

The Nodes
The worker nodes are the machines that run your containerized application.
the task of running, monitoring, and providing services to your application is done by the following components 
	Docker, rkt, or another container runtime, which runs your containers
	the Kubelet, which talks to the API server and manages containers on its node
	the Kubernetes Service Proxy (kube-proxy), which load-balances network traffic between application components

Running an application in Kubernetes
to run an application in Kubernetes, you first need to package it up into one or more container images,
push those images to an image registry,
and then post a description of your app to the Kubernetes API server
The description includes information such as the container image or images that contain your application components,how those components are related to each other, and which ones need to be run co-located (together on the same node) and which don’t. For each component, you can also specify how many copies (or replicas) you want to run. Additionally, the description also includes which of those components provide a service to either internal or external clients and should be exposed through a single IP address and made discoverable to the other components.

Understanding how the description results in a running container
When the API server processes your app’s description, the Scheduler schedules the specified groups of containers onto the available worker nodes based on computational resources required by each group and the unallocated resources on each node at that moment.
The Kubelet on those nodes then instructs the Container Runtime (Docker, for example) to pull the required container images and run the containers.
pods
a set of containers -> pod ; node run a pod or pods ; pull the container images and run the containers

Keeping the
Kubernetes continuously makes sure that the deployed state of the application always matches the description you provided.
instance stop , restart ; node dies , select new node

Scaling the number of copies
automatically keep adjusting the number, based on real-time metrics, 
such as CPU load, memory consumption, queries per second, or any other metric your app exposes.

Hitting a moving terget
To allow clients to easily find containers that provide a specific service, you can tell Kubernetes which containers provide the same service and Kubernetes will expose all of them at a single static IP address and expose that address to all applications running in the cluster.
This is done through environment variables, but clients can also look up the service IP through good old DNS.
The kube-proxy will make sure connections to the service are load balanced across all the containers that provide the service.
The IP address of the service stays constant, so clients can always connect to its containers, even when they’re moved around the cluster.

Understanding the benefits of using kubernetes

Simplifying application deployment
in essence, all the nodes are now a single bunch of computional resources that are waiting fro application to comsume them

Achieving better utilization of hardware
decoupled your app from the infrastructure

Health checking and self-healing

Automatic scaling

Simplifying application development

Summary
	monolithic apps are easier to deploy, but harder to maintain over time and sometimes impossible to scale
	microservices-based application architectures allow easier development of each component, but are harder to deploy and configure to work as single system
	Linux containers provide much the same benefits as virtual machines, but are far more lightweight and allow for much better hardware utilization
	Docker improved on existing Linux container technologies by allowing easier and faster provisioning of containerized apps together with their OS environments
	Kubernetes exposes the whole datacenter as a single computational resource for running applications
	developers can deploy apps through Kubernetes without assistence from sysadmins
	sysadmin can sleep better by having Kubernetes deal with failed nodes automatically


CH02 First steps with Docker and Kubernetes
covers
creating, running, and sharing a container image with Docker
running a single-node kubernetes cluster locally 
setting up a kubernetes cluster on google kubernetes engine
setting up and using the kubectl command-line client
deploying an app on kubernetes and scaling it horizontally

Creating , running , and sharing a container image
running application in Kubernetes requires them to be packaged into container images
	1. install Docker and run your first "Hello world" contaienr
	2. create a trivial Node.js app that you'll later deploy in Kubernetes
	3. Package the app into a container image so you can then run it as an isolated container
	4. run a container based on the image
	5. push the image to Docker Hub so that anyone anywhere can run it

Install Docker an running a Hello World container
if you're using a Mac or Windows and install Docker per instructions, Docker will set up a VM fro you and run the Docker daemon inside that VM.
busybox image, echo "Hello world"
busybox is a single executable that combines many of the standard UNIX command-line tools, such as echo,ls,gzip, and so on

$ docker run busybox echo "Hello world"

Understanding what happens behind the scenes
	1. docker run busybox echo "hello world"
	2. Docker check if busybox image is already stored locally
	3. docker pulls busybox image from registry(if not available locally)
	4. docker runs echo "hello world" in isolated container

Running other images
the command that should be executed is usually baked into the image itself, but you can override it if you want.

$ docker run <image>

Versioning container images
when referring to images without explicitly specifying the tag, Docker will assume you're referring to the so-called latest tag

$ docker run <image>:<tag>

Creating a trivial Node.js app
Node.js web application and package it into a container iamge
the application will accept HTTP requests and respond with the hostname of the machine it's running in
app running inside a container see its own hostname and not that of the host machine, even though it's running on the host like any other process
later, when you deploy the app on kubernetes and scale it out (scale it horizontally; that is run multiple instances of the app ). you'll see your HTTP request hitting different instances of the app
app.js
it starts up an HTTP server on port 8080.
the serever responds with an HTTP response status code 200 OK and the text "you've hit <hostname>" to every request. the request handler also logs the client's IP address to the standard output, later
the returned hostname is the server's actual hostname, not the on the client send in the HTTP request's Host hander
use Docker to package the app into a container image and enable to be run anywhere without having to download or install anything(except Docker, )





