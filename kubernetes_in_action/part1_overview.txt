1. Introducing Kubernetes

monoliths
legacy system
slow release cycles and are updated relatively infrequently
developers package up the whole system and hand it over to the ops teams

microservices
microservices are decoupled from each other, they can be developed, deployed , updated , and scaled individually
But with bigger numbers of deployable components and increasingly larger datacenters, it becomes increasingly difficult to configure, manage, and keep the wholesystem running smoothly.
resource utilization and hardware costs
automation

abstracts away the hardware infrastructure
deploy and run 
When deploying a multi-component application through Kubernetes, it selects a server for each component, deploys it, and enables it to easily find and communicate with all the other components of your application.

largest datacenters , such as the ones built and operated by cloud providers

Changes
a consequence of splitting big monolithic apps into smaller microservices
and the change in the infrastructure that runs those apps

Moving from monolithc apps to microservices
develop, deployed , and managed as one entity . run as a single OS process
Changes to one part fo the application require a redeployment of the whole application
certain parts of an application are extremely hard or next to impossible to scale horizontally

Splitting apps into microservices
each microservices run as an independent process and communicates with other microservices through simple, well-define interfaces
API

Scaling microservices
a per-serivce basis
which means you have the option of scaling only those services  that require more resources, while leaving others at their original scale
splitting the app into microservices allow you to horizontally scale the parts that allow scaling out

Deploying microservices
microservices also have drawbacks
components increases
making it hard to debug and trace execution calls
distributed tracing system such as Zipkin

Understanding the divergence of environment requirements
the bigger the number of components you need to deploy on the same host, the harder it will be to manage all  their dependencies to satisfy all their requirement

Providing a consistent environment ot applications
huge difference between development and production environment, difference even exist between individual production machines
developers often take care of their development laptops on their own

Moving to continuous delivery : DevOps and NoOps
the same team that develops the application also take part in deploying it and taking care of it over whole lifetime
this means the developer, QA, and operations teams now need to collaborate throughtout the whole process. this practice is called DevOps

Understanding the benefits
release newer versions of application nore often

Letting developers and sysadmins do what they do best
Kubernetes enables us to achieve all of this. By abstracting away the actual hardware and exposing it as a single platform for deploying and running apps, it allows developers to configure and deploy their applications without any help from the sysadmins and allows the sysadmins to focus on keeping the underlying infrastructure up and running, while not having to know anything about the actual applications running on top of it.

Introducing container technologies
kubernetes uses Linux container technologies to provide isolate of runing application, 
Kubernetes, Docker,  rkt

Understanding what containers are
only smaller numbers of large components. Virtual Machine (VM)
But when these components start getting smaller and their numbers start to grow

Isolating components with Linux container technologies
Instead of using virtual machines to isolate the environments of each microservice (or software processes in general), developers are turning to Linux container technologies.
run multiple services on the same host machine
not only exposing a different environment to each of them, but also isolating them from each other
A process running in a container runs inside the host’s operating system, like all the other processes
VMs, where processes run in separate operating systems
But the process in the container is still isolated from other processes.

Comparing virtual machine to containers
coantainers are much more lightweight
nothing more than a single isolated process running in the host OS, consuming only the resources that the app consumes and without the overhead of any additional processes
you often end up grouping multiple applications into each VM because you don’t have enough resources to dedicate a whole VM to each app. 
When using containers, you can (and should) have one container for each application

the main benefit of virtual machines is the full isolation they provide, because each VM runs its own Linux kernel,
while containers all call out to the same kernel, which can clearly pose a security risk

introducing the mechanisms that make container isolation possible
the first one ,Linux Namespaces, makes sure each process see its own personal view of the system (file,process, network interfaces,hostname)
the second on is Linux Control Groups (cgroups) , which limit the amount of resources the process can consume(CPU,memory,network bandwith)

Isolating processes with Linux Namespaces
by default, each Linux system initially has one single namespace.
All system resources, such as filesystems, process IDs, user IDs, network interfaces, and others, belong to the single namespace.
But you can create additional namespaces and organize resources across them.
When running a process, you run it inside one of those namespaces. The process will only see resources that are inside the same namespace.
a process doesn’t belong to one namespace, but to one namespace of each kind.
namespace of each kind.
The following kinds of namespaces exist:
	Mount (mnt)
	Process ID (pid)
	Network (net)
	Inter-process communication (ipc)
	UTS
	User ID (user)
each namespace kind is used to isolate a certain group of resources
By assigning two different UTS namespaces to a pair of processes, you can make them see different local hostnames.

Limiting resource available to a process
cgroups, a Linux kernel feature that limits the resource usage of a process(or a group of processes)
a processes can't use more than the configured amount of CPU,memory,network bandwidth
This way, processes cannot hog resources reserved for other processes, which is similar to when each process runs on a separate machine.

Introducing the Docker container platform
It simplified the process of packaging up not only the application but also all its libraries and other dependencies, even the whole OS file system, into a simple, portable package that can be used to provision the application to any other machine running Docker.
a big different between Docker-based container images and VM images is that container images are composed of layers, which can be shared and reused across multiple images
This means only certain layers of an image need to be downloaded if the other layers were already downloaded previously when running a different container image that also contains the same layers

Understanding Docker concepts
Docker is a platform for packaging,distributing,and running applications
three main concepts in Docker comprise this scenario
Images : 
A Docker-based container image is something you package your application and its environment into.
it contains the filesystem that will be available to the application and other metadata, such as the path to the executable that should be executed when the image is run
Register :
A Docker Registry is a repository that stores your Dokcer images and facilitates easy sharing of the images between different people and computers
Containers :
A Docker-based container is a regular Linux container created from a Docker-based container image.
A running container is a process running on the host running Docker, but it’s completely isolated from both the host and all other processes running on it.

Building, Distributing, and Running a Docker image
1. developer tells Docker to build and push image
2. Docker builds image
3. Docker pushes image to registry
4. developer tells Docker on production machine to run image
5. Docker pulls image from registry
6. Docker runs container from image

Comparing virtual machine and Dokcer contianers
each container has its own isolated filesystem . how can both them share the same files?

Understanding image Layers
Docker images are composed of layers
different images can contain the exact same layer because every Docker image is built on top of another image and two different images can both use the parent image as their base
But layers don’t only make distribution more efficient, they also help reduce the storage footprint of images. Each layer is only stored once.
Two containers created from two images based on the same base layers can therefore read the same files, but if one of them writes over those files, the other one doesn’t see those changes.
Therefore, even if they share files, they’re still isolated from each other. This works because container image layers are read-only.
When a container is run, a new writable layer is created on top of the layers in the image. When the process in the container writes to a file located in one of the underlying layers, a copy of the whole file is created in the top-most layer and the process writes to the copy.

Understanding the portability limitations of container images







