1. Introducing Kubernetes

monoliths
legacy system
slow release cycles and are updated relatively infrequently
developers package up the whole system and hand it over to the ops teams

microservices
microservices are decoupled from each other, they can be developed, deployed , updated , and scaled individually
But with bigger numbers of deployable components and increasingly larger datacenters, it becomes increasingly difficult to configure, manage, and keep the wholesystem running smoothly.
resource utilization and hardware costs
automation

abstracts away the hardware infrastructure
deploy and run 
When deploying a multi-component application through Kubernetes, it selects a server for each component, deploys it, and enables it to easily find and communicate with all the other components of your application.

largest datacenters , such as the ones built and operated by cloud providers

Changes
a consequence of splitting big monolithic apps into smaller microservices
and the change in the infrastructure that runs those apps

Moving from monolithc apps to microservices
develop, deployed , and managed as one entity . run as a single OS process
Changes to one part fo the application require a redeployment of the whole application
certain parts of an application are extremely hard or next to impossible to scale horizontally

Splitting apps into microservices
each microservices run as an independent process and communicates with other microservices through simple, well-define interfaces
API

Scaling microservices
a per-serivce basis
which means you have the option of scaling only those services  that require more resources, while leaving others at their original scale
splitting the app into microservices allow you to horizontally scale the parts that allow scaling out

Deploying microservices
microservices also have drawbacks
components increases
making it hard to debug and trace execution calls
distributed tracing system such as Zipkin

Understanding the divergence of environment requirements
the bigger the number of components you need to deploy on the same host, the harder it will be to manage all  their dependencies to satisfy all their requirement

Providing a consistent environment ot applications
huge difference between development and production environment, difference even exist between individual production machines
developers often take care of their development laptops on their own

Moving to continuous delivery : DevOps and NoOps
the same team that develops the application also take part in deploying it and taking care of it over whole lifetime
this means the developer, QA, and operations teams now need to collaborate throughtout the whole process. this practice is called DevOps

Understanding the benefits
release newer versions of application nore often

Letting developers and sysadmins do what they do best
Kubernetes enables us to achieve all of this. By abstracting away the actual hardware and exposing it as a single platform for deploying and running apps, it allows developers to configure and deploy their applications without any help from the sysadmins and allows the sysadmins to focus on keeping the underlying infrastructure up and running, while not having to know anything about the actual applications running on top of it.

Introducing container technologies
kubernetes uses Linux container technologies to provide isolate of runing application, 
Kubernetes, Docker,  rkt

Understanding what containers are
only smaller numbers of large components. Virtual Machine (VM)
But when these components start getting smaller and their numbers start to grow

Isolating components with Linux container technologies
Instead of using virtual machines to isolate the environments of each microservice (or software processes in general), developers are turning to Linux container technologies.
run multiple services on the same host machine
not only exposing a different environment to each of them, but also isolating them from each other
A process running in a container runs inside the host’s operating system, like all the other processes
VMs, where processes run in separate operating systems
But the process in the container is still isolated from other processes.

Comparing virtual machine to containers
coantainers are much more lightweight
nothing more than a single isolated process running in the host OS, consuming only the resources that the app consumes and without the overhead of any additional processes
you often end up grouping multiple applications into each VM because you don’t have enough resources to dedicate a whole VM to each app. 
When using containers, you can (and should) have one container for each application

the main benefit of virtual machines is the full isolation they provide, because each VM runs its own Linux kernel,
while containers all call out to the same kernel, which can clearly pose a security risk

introducing the mechanisms that make container isolation possible
the first one ,Linux Namespaces, makes sure each process see its own personal view of the system (file,process, network interfaces,hostname)
the second on is Linux Control Groups (cgroups) , which limit the amount of resources the process can consume(CPU,memory,network bandwith)

Isolating processes with Linux Namespaces
by default, each Linux system initially has one single namespace.
All system resources, such as filesystems, process IDs, user IDs, network interfaces, and others, belong to the single namespace.
But you can create additional namespaces and organize resources across them.
When running a process, you run it inside one of those namespaces. The process will only see resources that are inside the same namespace.
a process doesn’t belong to one namespace, but to one namespace of each kind.
namespace of each kind.
The following kinds of namespaces exist:
	Mount (mnt)
	Process ID (pid)
	Network (net)
	Inter-process communication (ipc)
	UTS
	User ID (user)
each namespace kind is used to isolate a certain group of resources
By assigning two different UTS namespaces to a pair of processes, you can make them see different local hostnames.

Limiting resource available to a process
cgroups, a Linux kernel feature that limits the resource usage of a process(or a group of processes)
a processes can't use more than the configured amount of CPU,memory,network bandwidth
This way, processes cannot hog resources reserved for other processes, which is similar to when each process runs on a separate machine.

Introducing the Docker container platform
It simplified the process of packaging up not only the application but also all its libraries and other dependencies, even the whole OS file system, into a simple, portable package that can be used to provision the application to any other machine running Docker.
a big different between Docker-based container images and VM images is that container images are composed of layers, which can be shared and reused across multiple images
This means only certain layers of an image need to be downloaded if the other layers were already downloaded previously when running a different container image that also contains the same layers

Understanding Docker concepts
Docker is a platform for packaging,distributing,and running applications
three main concepts in Docker comprise this scenario
Images : 
A Docker-based container image is something you package your application and its environment into.
it contains the filesystem that will be available to the application and other metadata, such as the path to the executable that should be executed when the image is run
Register :
A Docker Registry is a repository that stores your Dokcer images and facilitates easy sharing of the images between different people and computers
Containers :
A Docker-based container is a regular Linux container created from a Docker-based container image.
A running container is a process running on the host running Docker, but it’s completely isolated from both the host and all other processes running on it.

Building, Distributing, and Running a Docker image
1. developer tells Docker to build and push image
2. Docker builds image
3. Docker pushes image to registry
4. developer tells Docker on production machine to run image
5. Docker pulls image from registry
6. Docker runs container from image

Comparing virtual machine and Dokcer contianers
each container has its own isolated filesystem . how can both them share the same files?

Understanding image Layers
Docker images are composed of layers
different images can contain the exact same layer because every Docker image is built on top of another image and two different images can both use the parent image as their base
But layers don’t only make distribution more efficient, they also help reduce the storage footprint of images. Each layer is only stored once.
Two containers created from two images based on the same base layers can therefore read the same files, but if one of them writes over those files, the other one doesn’t see those changes.
Therefore, even if they share files, they’re still isolated from each other. This works because container image layers are read-only.
When a container is run, a new writable layer is created on top of the layers in the image. When the process in the container writes to a file located in one of the underlying layers, a copy of the whole file is created in the top-most layer and the process writes to the copy.

Understanding the portability limitations of container images
one small caveat exists : all containers running on a host use the host's Linux kernel
the different version of the Linux kernel
the different hardware architecture (x86,ARM-based)

Intoducing rkt - an alternative to Docker

Introducing Kubernetes
as the number of deployable application components in your system grows, it becomes harder to manage them all.
This has forced them to develop solutions for making the development and deployment of thousands of software components manageable and cost-efficient.

Understanding its origins
help both application developers and system administrators manage those thousands of applications and services.
After having kept Borg and Omega secret for a whole decade, in 2014 Google introduced Kubernetes, an open-source system based on the experience gained through Borg, Omega, and other internal Google systems.

Looking at Kubernetes from the top of a mountain
Kubernetes is a software system that allows you to easily deploy and manage containerized applications on top of it.
without having to know any internal details of these applications and without having to manually deploy these applications on each host.
This is of paramount importance for cloud providers, because they strive for the best possible utilization of their hardware while still having to maintain complete isolation of hosted applications.
Kubernetes enables you to run your software applications on thousands of computer nodes as if all those nodes were a single, enormous computer.
It abstracts away the underlying infrastructure and, by doing so, simplifies development, deployment, and management for both development and the operations teams.
Deploying applications through Kubernetes is always the same, whether your cluster contains only a couple of nodes or thousands of them.
Additional cluster nodes simply represent an additional amount of resources available to deployed apps.

Understanding the core of what kubernetes does
developer -> App descriptor -> Kubernetes master -> Tens or thousands of worker nodes exposed as a single deployment platform
a master node and any number of worker nodes
when the developer submits a list of apps to the master, kubernetes deploys them to the cluster of worke nodes. What node a component lands on doesn't (and shouldn't) matter
the developer can specify that certain apps must run together and Kubernetes will deploy them on the same worker node. Other will be spread around the cluster, but they can talk to each other in the same way, regardless of where they're deployed

Helping developers focus on the core app feature
Kubernetes can be thought of as an operating system for the cluster.
infrastructure-related services
Kubernetes to provide these services.
this includes things such as service discovery, scaling, load-balancing, self-healing, and even leader election

Helping OPS teams achieve better resource utilization
Kubernetes will run your containerized app somewhere in the cluster, provide information to its components on how to find each other, and keep all of them running.
Because your application doesn’t care which node it’s running on, Kubernetes can relocate the app at any time, and by mixing and matching apps, achieve far better resource utilization than is possible with manual scheduling.

Understanding the architecture of a Kubernetes cluster
at the hardware level, a kubernetes cluster is composed of many nodes, which can be split into two types
	the master node, which hosts the kubernetes Control Plane that controls and manages the whole Kubernetes system
	Worker nodes that run the actual application you deploy
Control Plane (master) :
etcd <- API server ; API server <- Schedule ; API server <- Controller Manager
Worker node(s) :
(master)API server <- Kubelet -> Container Runtime ; (master)API server <- kube-proxy

The Control Plane
The Control Plane is what controls the cluster and makes it functions.
in consist of multiple components that can run on a single master node or be split across multiple nodes and replicated to ensure high availability. these components are
	The Kubernetes API server, which you and the other Control Plane componenets communicate with
	The Schedule, which schedules your apps (assigns a worker node to each deployable component of your application)
	The Control Manager, which performs cluster-level functions, such as replicating components, keeping track of worker nodes, handling node failures , and so on
	etcd, a reliable distributed data store that persistently stores the cluster configuration
the components of the Control Plane hold and control the state of the cluster, but they don't run your applications. 

The Nodes
The worker nodes are the machines that run your containerized application.
the task of running, monitoring, and providing services to your application is done by the following components 
	Docker, rkt, or another container runtime, which runs your containers
	the Kubelet, which talks to the API server and manages containers on its node
	the Kubernetes Service Proxy (kube-proxy), which load-balances network traffic between application components

Running an application in Kubernetes
to run an application in Kubernetes, you first need to package it up into one or more container images,
push those images to an image registry,
and then post a description of your app to the Kubernetes API server
The description includes information such as the container image or images that contain your application components,how those components are related to each other, and which ones need to be run co-located (together on the same node) and which don’t. For each component, you can also specify how many copies (or replicas) you want to run. Additionally, the description also includes which of those components provide a service to either internal or external clients and should be exposed through a single IP address and made discoverable to the other components.

Understanding how the description results in a running container
When the API server processes your app’s description, the Scheduler schedules the specified groups of containers onto the available worker nodes based on computational resources required by each group and the unallocated resources on each node at that moment.
The Kubelet on those nodes then instructs the Container Runtime (Docker, for example) to pull the required container images and run the containers.
pods
a set of containers -> pod ; node run a pod or pods ; pull the container images and run the containers

Keeping the
Kubernetes continuously makes sure that the deployed state of the application always matches the description you provided.
instance stop , restart ; node dies , select new node

Scaling the number of copies
automatically keep adjusting the number, based on real-time metrics, 
such as CPU load, memory consumption, queries per second, or any other metric your app exposes.

Hitting a moving terget
To allow clients to easily find containers that provide a specific service, you can tell Kubernetes which containers provide the same service and Kubernetes will expose all of them at a single static IP address and expose that address to all applications running in the cluster.
This is done through environment variables, but clients can also look up the service IP through good old DNS.
The kube-proxy will make sure connections to the service are load balanced across all the containers that provide the service.
The IP address of the service stays constant, so clients can always connect to its containers, even when they’re moved around the cluster.

Understanding the benefits of using kubernetes

Simplifying application deployment
in essence, all the nodes are now a single bunch of computional resources that are waiting fro application to comsume them

Achieving better utilization of hardware
decoupled your app from the infrastructure

Health checking and self-healing

Automatic scaling

Simplifying application development

Summary
	monolithic apps are easier to deploy, but harder to maintain over time and sometimes impossible to scale
	microservices-based application architectures allow easier development of each component, but are harder to deploy and configure to work as single system
	Linux containers provide much the same benefits as virtual machines, but are far more lightweight and allow for much better hardware utilization
	Docker improved on existing Linux container technologies by allowing easier and faster provisioning of containerized apps together with their OS environments
	Kubernetes exposes the whole datacenter as a single computational resource for running applications
	developers can deploy apps through Kubernetes without assistence from sysadmins
	sysadmin can sleep better by having Kubernetes deal with failed nodes automatically


CH02 First steps with Docker and Kubernetes
covers
creating, running, and sharing a container image with Docker
running a single-node kubernetes cluster locally 
setting up a kubernetes cluster on google kubernetes engine
setting up and using the kubectl command-line client
deploying an app on kubernetes and scaling it horizontally

Creating , running , and sharing a container image
running application in Kubernetes requires them to be packaged into container images
	1. install Docker and run your first "Hello world" contaienr
	2. create a trivial Node.js app that you'll later deploy in Kubernetes
	3. Package the app into a container image so you can then run it as an isolated container
	4. run a container based on the image
	5. push the image to Docker Hub so that anyone anywhere can run it

Install Docker an running a Hello World container
if you're using a Mac or Windows and install Docker per instructions, Docker will set up a VM fro you and run the Docker daemon inside that VM.
busybox image, echo "Hello world"
busybox is a single executable that combines many of the standard UNIX command-line tools, such as echo,ls,gzip, and so on

$ docker run busybox echo "Hello world"

Understanding what happens behind the scenes
	1. docker run busybox echo "hello world"
	2. Docker check if busybox image is already stored locally
	3. docker pulls busybox image from registry(if not available locally)
	4. docker runs echo "hello world" in isolated container

Running other images
the command that should be executed is usually baked into the image itself, but you can override it if you want.

$ docker run <image>

Versioning container images
when referring to images without explicitly specifying the tag, Docker will assume you're referring to the so-called latest tag

$ docker run <image>:<tag>

Creating a trivial Node.js app
Node.js web application and package it into a container iamge
the application will accept HTTP requests and respond with the hostname of the machine it's running in
app running inside a container see its own hostname and not that of the host machine, even though it's running on the host like any other process
later, when you deploy the app on kubernetes and scale it out (scale it horizontally; that is run multiple instances of the app ). you'll see your HTTP request hitting different instances of the app
	ch02/app.js
it starts up an HTTP server on port 8080.
the serever responds with an HTTP response status code 200 OK and the text "you've hit <hostname>" to every request. the request handler also logs the client's IP address to the standard output, later
the returned hostname is the server's actual hostname, not the on the client send in the HTTP request's Host hander
use Docker to package the app into a container image and enable to be run anywhere without having to download or install anything(except Docker, )

Creating Dockerfile for the image
to package your app into an image, create file called Dockerfile, which will contain a list of instructions that Docker will perform when building the image
the Dockerfile needs to be in the same directory as the app.js file and should contain the commands shown in the following listing
	ch02/Dockerfile
the FROM line defines the container image you'll use as a starting point( the base image you're building on top of)
using the node container image, tag 7
second line , adding app.js file from local directory into the root directory in the image, under the same name (app.js)
third line, defining the what command should be executed when somebody runs the image
the node image is made specifically for running Node.js apps

Build the container image
Dockerfile and the app.js file, command (pwd /home/liuh/go/go_portfolio/kubernetes_in_action/ch02)

$ docker build -t kubia .

build an image called kubia based on the cotents of the current directory ( note the dot at the end of the build command )
	1. docker build kubia . -> Docker client
	2. Docker client uploads directory contents to daemon (Dockerfile app.js)
	Docker daemon node:7.0 -> kubia:latest
	3. Docker pulls image node:7.0 if it isn't stored locally yet
	4. build new image

Understanding how an image is built
the client and daemon don't need to be on the same machine at all. If you're using Docker on a non-Linux OS, the client is on your host OS, but the daemon runs inside a VM
all the files in the build directory are uploaded to the daemon, if ti contains many large files and the daemon isn't running locally, the upload may take longer

Understand image layers
An image isn't single, big, binary blob, but is composed of multiple layers, which you may have already noticed when running the busybox example (there were multiple Pull complete lines-one of each layer)
different images may share several layers, which makes storing and transferring images much more efficient
if you create multiple images based on the same base image (such as node:7 in the example), all the layers comprising the base image will be stored only once.
when pulling an image, Docker will download each layer individually.
several layers may already be stored on your machine, so Docker will only download those that aren't
When building an image, a new layer is created for each individual command in the Dokcerfile.
during the build of your image, after pulling all the layers of the base image, Docker will create a newe layer on top of them and add the app.js file into ti
then it will create yet another layer that will spcify the command that should be excuted when the image is run
this last layer will then be tagged as kubia:latest
kubia:latest image : buildpack-deps:jessie image ; node:0.12 image  other:latest image

$ docker images

Comparing building images with a Dokcer file va manually
Dockerfile, automatically and repeatable

Running the container image

$ docker run --name kubia-container -p 8080:8080 -d kubia

run a new container called kubia-container from the kubia image.
the container will be detached from the console (-d flag), which means it will run in the background
port 8080 on the local machine will be mapped on port 8080 inside the container (-p 8080:8080)
if you're not running the Docker daemon on your local machine, you'll need to use the hostname or IP of the VM running the daemon instead of localhost.
http://localhost:8080

Accessing your app

$ curl localhost:8080
you've hit 883e3b443576

that's the reponse from your app. it is running inside a container, isoldated from everything else
883e3b443576 is its hostname, and not the actual hostname of your host machine . ID of the docker container

Listring all running container

$ docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                    NAMES
883e3b443576        kubia               "node app.js"       4 hours ago         Up 4 hours          0.0.0.0:8080->8080/tcp   container

Getting additional information about a container
to see additional information , you can use docker inspect

$ docker inspect  kubia-container

Docker will print out a long JSON containeing low-level information about the container

Exploring the inside of a running container
what the environment is like inside the container

Running a shell inside an existing container

$ docker exec -it kubia-container bash

this will run bash inside the existing kubia-container container.
the bash process will have the same Linux namespace as the main container process
this allows you to explore the container from within and see how Node.js and your app see the system when running inside the container. the -it option
	-i, which makes sure STDIN is kept open. you need this fro entering commands into the shell
	-t, which allocates a pseudo terminal (TTY)
you need both if you want the use the shell like you're  used to 
(if you leave out the first one , you can't type any commands, and if you leave out the second one, the command prompt won't be displayed and some commands will complian about the TERM variable not being set)

Exploring the container from within
use the shell in the following listint to see the processes running in the contaienr

root@883e3b443576:/# ps aux
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.4 813604 25616 ?        Ssl  14:10   0:00 node app.js
root        13  0.0  0.0  20248  3128 pts/0    Ss   18:46   0:00 bash
root        25  0.0  0.0  17504  2004 pts/0    R+   18:53   0:00 ps aux

three processes. 

Understanding that processes in a container run it the host operating system
open another terminal and list the processes on the host OS itself

liuh@liuh-pc:~$ ps aux | grep app.js
root     22906  0.0  0.4 813604 25616 ?        Ssl   8月05   0:00 node app.js

this proves that processes running in the container are running in the host OS
hte processes have different IDs inside the container vs. on the host
the container is using its own PID Linux namespace and has a completely isolated process tree, with its own sequence of numbers

The container's filesystem is also isolated
like having an isolated precess tree, each container also has an isolated filesystem

root@883e3b443576:/# ls
app.js	bin  boot  dev	etc  home  lib	lib64  media  mnt  opt	proc  root  run  sbin  srv  sys  tmp  usr  var

Entering a running contaienr like this is useful when debugging an app running in a container
an application will not see its own unique filesystem, but alse processes, users, hostname, and network interfaces

Stopping and removing a container

$ docker stop kubia-container

this will stop the main process running in the container and consequently stop the container, because no other processes are running inside the container
the container itself still exists and you can see it with (docker ps -a). the -a option prints out all the  containers , those running  and those that have benen stopped 
to truly remove a container , you need to remove it with th docker rm command

$ docker rm kubia-container

Pushing the image to an image registry
Docker Hub (http://hub.docker.com), which is one fo the publicly available registries
Quay.io and Google container Registry
before you do that, you need to re-tag your image according to Docker Hub's rules.
Docker Hub will allow you to push an image if the  image's repository name starts with your Docker Hub

Tagging an image under an additional tag
once you know your ID (844348677) , you're ready to rename your image, currently tagged as kubia, to 844348677/kubia 

$ docker tag kubia 844348677/kubia

this doesn't rename the tag; it creates an additional tag fro the same image

liuh@liuh-pc:~/go/go_portfolio/kubernetes_in_action/ch02$ sudo docker images |head
REPOSITORY                                     TAG                 IMAGE ID            CREATED             SIZE
844348677/kubia                                latest              febbdf6cf0b1        7 hours ago         660MB
kubia                                          latest              febbdf6cf0b1        7 hours ago         660MB
busybox                                        latest              e1ddd7948a1c        4 days ago          1.16MB
844348677/testserver                           latest              0411d0d1b02c        8 days ago          4.39MB
testserver                                     latest              0411d0d1b02c        8 days ago          4.39MB

both kubia and 844348677/kubia point to the same image ID, so they're in fact one single image with two tags

Pushing the image to Docker Hub
before you can push the image to Docker Hub, you need to log in under your user ID with the docker login command

$ docker login
$ docker push 844348677/testserver
$ docker push 844348677/kubia

Running the image on a different machine

$ docker run -p 8080:8080 -d 844348677/kubia

the best thing about is that your application will have the exact same environment every time and everywhere it's run

Setting up a Kubernetes cluster
setting up a full-fledged, multi-node Kubernetes cluster isn't a simple task,
the documentation at http://kubernetes.io
Kubernetes can be run on your local development machine, your own organization's cluster of machines, on cloud providers providing virtual machines (Google Compute Engine, Amazon EC2, Microsoft Azure, and so on), or by using a  managed Kubernetes cluster such as Google Kubernetes Engine(previously known a as Google contaienr Engine)
run a single-node Kubernetes cluster on your local machine 

Running a local single-node Kubernetes cluster with Minikube
the simplest and quickest path to a fully functioning Kubernetes cluster is by using Minikube
Minikube is a tool that sets up a single-node cluster that's great for both testing Kubernetes and developing apps locally

Installing Minikube
Minikube is a single binary that needs to be download and put onto your path.
http://github.com/kubernetes/minikube
Install a Hypervisor
For Linux, install VirtualBox or KVM.
https://www.virtualbox.org/wiki/Downloads
https://www.linux-kvm.org/page/Main_Page
Note: Minikube also supports a --vm-driver=none option that runs the Kubernetes components on the host and not in a VM. Docker is required to use this driver but a hypervisor is not required.
--vm-driver=none
Install kubectl
https://kubernetes.io/docs/tasks/tools/install-kubectl/
Install kubectl binary via curl
1. Download the latest release with the command:
$ curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
! www.google.com GFW
$ scp -P xxxxx root@xxx.xxx.xxx.xxx:/root/kubectl .
2. Make the kubectl binary executable.
$ chmod +x ./kubectl
3. Move the binary in to your PATH.
$ sudo mv ./kubectl /usr/local/bin/kubectl
Install Minikube
$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube && sudo cp minikube /usr/local/bin/ && rm minikube
$ scp -P xxxxx root@xxx.xxx.xxx.xxx:/root/minikube .

Staring a Kubernetes cluster with Minikube

$ minikube start

$ minikube --vm-driver=none start

Installing the kubernetes client (kubectl)
to interact with kubernetes , you also need the kubectl CLI client

Checking to see the cluster is up and Kubectl can talk to it
to verify your cluster is working, you can use the kubectl cluster-info command 
$ kubectl cluster-info

you can run minikube ssh to log into the Minikube VM and explore it from the inside.

Using a hosted Kubernetes cluster with Google Kubernetes Engine

Setting up a Google Cloud Project and download the necessary client binaries
the whole procedure includes
	1. Signing up for a Google account,
	2. creating a project in the Google Cloud Platform Console
	3. Enabling billing. this does require your credit card info, but Google provides a 12-month free tail. and they're nice enough to not start charging automatically after the free trial is over
	4. Enabling the Kubernetes Engine API
	5. downloading and installing Google Gloud SDK (this includes the gcloud command-line tool,which you'll need to create a Kubernetes cluster)
	6. Installing the kubectl command-line tool with gcloud components install kubectl

Creating a Kubernetes cluster with three nodes

Getting an overview of your cluster
each node runs Docker, the Kubelet and  the kube-proxy

local dev machine (kubectl) -- (REST call) --> Kubernetes cluster (REST API server) master node
worker nodes , (Docker, Kubelet, kube-proxy)

Checking if the cluster  is up yb listing cluster nodes
use the kubectl command now to list all the nodes in your cluster

kubectl get nodes

Retrieving additional details of an object
you can use the kubectl describe command, 

$ kubectl describe node 

Setting up an alias and command-line completion for kubectl

Creating an alias

Configuring tab completion for kubectl

Running your first app on Kubernetes

Deploying your Node.js app
kubectl run

$ kubectl run kubia --image=844348677/kubia --port=8080 --generator=run/v1

the --image=844348677/kubia , --port=8080 , the last flag (--generator) does require an explanation, so Kubernetes creates a ReplicationController instead of a Deployment
a ReplicationController called kubia has been created





