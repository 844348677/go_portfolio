ch03 Pods:running containers in Kubernetes
	creating, running, and stopping pods
	organizing pods and other resources with labels
	performing an operation on all pods with a specific label
	using namespaces to split pods into non-overlapping groups
	scheduling pods onto specific types of worker nodes

Introducing pods
when a pod does contain multiple containers, all of them are always run on a single worker node, it never spans multiple worker nodes

Understanding why we need pods

Understanding why multiple containers are better than one container running multiple precess
containers are designed to run only a single process per container (unless the process itself spawn)

Understanding pods
you need another higher-level construct that will allow you to bind containers together and manage them as a single unit
almost the same environment as if ther were all running in a single container, while keeping them somewhat isolated
you can take advangtage of all the features containers provide, while at the same time giving the processes the illusion of running together

Understanding the partial isolation between containers of the same pod
you want to isolate groups of containers instead of individual ones
Kubernetes achieves this by configuring Docker to have all container of a pod share the same set of Linux namespaces instead of each container having its own set
Because all containers of a pod run under the same Network and UTS namespaces(Linux namespace here), they all share the same hostname and network interfaces
similarly, all container of a pod run under the same IPC namespace and can communicate through IPC (Inter-Process Communication), and can communication through IPC. (PID namespace)
when containers of the same pod use seperate PID namespaces, you only see the contaienr's own processes when running ps aux in the container
But when ti comes to the filesystem, things are a little different.
because most of the container's filesystem comes from the container image, by default, the filesystem of each container is fully isolated from other containers
using a Kubernetes concept called a Volume

Understanding how containers share the same IP and PORT space
because containers in a pod run in the same Network namespace, they share the same IP address and port space
ports conflicts

Introducing the flat inter-pod network
every pod can access every other pod at the other pod's IP address
communication between pods is always simple
it doesn't matter if two pod are sheduled onto a single or onto different worker nodes
containers inside those pods can communicate with each other across the flat NAT-less (Network Address Translation) network, much like computers on a local area network (LAN)
pods are logical hosts and behave much like physical hosts or VMs in the non-container world

Organizing containers across pods properly
you should think of pods as seperate mahines
unlike the old days, when we used to cram all sorts of apps onto the same hosts
you should organize app into multiple pods, where each one contains only tightly related components or process

Splitting multi-tier apps into multiple pods

Splitting into multiple pods to enable individual scaling
Kubernetes can't horizontally scale individual containers; instead, it scales whole pods

Understanding when to use multiple containers in a pod
wehn the application consists of one process and one or more complementary process
Pods should contain tightly coupled containers, usually a main container and contianers that support the main one
for example, the main container in a pod could be a web server that serves files from a certain file directory, while an additional container (a sidecar container) periodically downloads content from an external source and stores it in the web server's directory
(use a Kubernetes Volume that you mount into both containers)

Deciding when to use multiple containers in a pod
recap how container should be grouped into pods
	Do they need to be run together or can they run on different hosts
	Do they represent a single whole or are they independent components
	Must they be scaled together or individually
a container shouldn't run multiple process. a pod shouldn't contain multiple containers if they don't need to run on the same machine

Creating pods from YAML or JSON descriptors
pods an other kubernetes resources are usually created by posting a JSON or YAML manifest to the Kubernetes REST API endpoint

Examining a YAML descriptor of an existing pod
you'll use the kubectl get command with the -o yaml option to get the whole YAML definition of the pod

$ kubectl get po kubia-zxzij -o yaml

Introducing the main parts of a pod definition
first, there's the Kubernetes API version used in the YAML and the type of resource the YAML is describing
then, three important section are found in almost all Kubernets resouces
	Metadata includes the name, namespace, labels, and other infromation about the pod
	Spec contains the actual description of the pod's contents, such as the pod's containers, volumes, and other data
	Status contains the current information about the running pod, such as what condition the pod is in, the description and status of each container, and the pod's inteernal IP and other basic info
the status part contains read-only runtime data that shows the state of the resource at a given moment.
ch03/kubia-manual.yaml

Creating a simple YAML descriptor for a pod
// Descriptor conforms to version v1 of Kubernetes API
apiVersion: V1
// you're describing a pod
kind: Pod
metadata:
	// the name of the pod
  name: kubia-manual
spec:
  containers:
  	// Container image to create the container from 
    - image: 844348677/kubia
    	// name of the contaienr
       name: kubia
       // the port the app is listening on
       ports:
       - containerPort: 8080
          protocol: TCP

Specifying container ports
it makes sense to define the ports explicitly so that everyone using your cluster can quickly see what ports each pod exposes
explicitly defining ports also allows you to assign a name to each port

$ kubectl explain pods
$ kubectl explain pod.spec

Using kubectl create to create the pod
to create the pod from your YAML file, use the kubectl create command

$ kubectl create -f kubia-manual.yaml

the kubectl create -f command is used fro creating any resource (not only pods) from a YAML or JSON file

Retrieving the whole definition of a running pod
after creating the pod, you can ask Kubernetes for the full YAML of the pod

$ kubectl get po kubia-manual -o yaml
$ kubectl get po kubia-manual -o json

Seeing your newly created pod in the list of pods

$ kubectl get pods

Viewing application logs
containerized application usually log to the standard output and standard error stream instead of writing their logs to files
the container runtime (Docker in your case) redirects those stream to files and allow you to get the container's log by running

$ docker logs <container id>

Retrieving a pod's log with kubectl logs

$ kubectl logs kubia-manual

Specifying the container name when getting logs fo a multi-container pod

$ kubectl logs kubia-manul -c kubia

centralized, cluster-wide logging, which stores all the logs into a central store

Sending requests to the pod
the kubectl expose command to create a service to gain access to the pod externally
other ways of connecting to a pod for testing and debugging purposes
one of them is through port forwarding

Forwarding a local network port to a port in the pod
when you want to talk to a specific pod without going through a service (for debugging or other reasonss), Kubernetes allows you to configure port forwarding to the pod
local port 8888 to port 8080 of your kubia-manual pod

$ kubectl port-forward kubia-manual 8888:8080

Connecting to the pod trough the port forwarder

$ curl localhost:8888

using port forwarding like this is an effective way to test an individual pod

Organizing pods with labels
as the number of pods increases, the need for categorizing them into subsets becomes more and more evident
microservices architectures
those components will porbably be replicated(multiple copies of the same componen will be deployed) and multiple versions or releases (stable, beta, canary, and so on) will run concurrently
you'll want to operate on every pod belonging to a certain group with a single action instead of having to perform the action for each pod  individually
organizing pods and all other Kubernetes objects is done through labels

Introducing labels
labels are simple, yet incredibly powerful, Kubernetes feature for organizing not only pods, but all other Kubernetes resources
a label is an arbitrary key-value pair you attach to a resource, which is then utilized when selecting resources using label selectors ( resources are filtered based on whether they include the label specified in the selector)
a resource can have more than on label, as long as the keys of the labels are unique within that resource.
you usually attach labels to resources when you create them, but you can also add additional labels or even modify the values of existing labels later without having to recreate the resource
by adding labels to those pods, you get a much-better-organized system that everyone can easily make sense of
	app, which specifies which app, component, or microservice the pod belongs to
	rel, which shows whether the application running in the pod is a stable, beta, or a canary release
by adding these two labels, you've essentially organized your pods into two dimensions (horizontally by app and vertically by release)

Specifying labels when creating a pod
labels in action by creating a new pod with two labels
ch03/kubia-manual-with-labels.yaml

apiVersion: v1
kind: Pod
metadata:
  name: kubia-manual-v2
  // two labels are attached to the pod
  labels:
    creation_method: manual 
    env: prod
spec:
  containers:
    - image: 844348677/kubia
       name: kubia
       ports:
       - containerPort: 8080
          protocol: TCP

the labels creatinon_method=manual and env=prod

$ kubectl create -f kubia-manul-with-labels.yaml
$ kubectl get po --show-labels

instead of listing all labels, if you're only interested in certain labels, you can specify them with the -L switch and have each displayed in its own column

$ kubectl get po -L creation_method,env

Modify labels of existing pods
labels can also be added to and modified on existing pods

$ kubectl label po kubia-manual creation_method=manula
$ kubectl label po kubia-manula-v2 env=debug --overwrite

you need to use the --overwrite option when changing existing labels

$ kubectl get po -L creation_method,env

Listing subsets of pods through label selectors
label selectors allow you to select a subset of pods tagged with certain labels and perform an operation on those pods
a label selector is a criterion, which filters resources based on whether they include a certian label with a certain value
	contains (or doesn't contain) a label with a certain key
	contains a label with a certain key and value
	contains a label with a certain key, but with a value not equal to the one you specify

Listing pods using a label selector

$ kubectl get po -l creation_method=manual
$ kubectl get po -l env
$ kubectl get po -l '!env'

	creation_method!=manual to select pods with the creation_method label  with any value other than manual
	env in (prod,devel) to select pods with the env label set to either prod or devel
	env notion (prod,devel) to select pods with the env label set to any value other than prod or devel

Using multiple condition in a label selector
a selector can also include multiple comma-separated criteria
label selectors aren't usefor only fro listing pods, but also for performing actions on a subset of all pods

Using labels and selectors to constrain pod scheduling
when your hardware infrastructure isn't homogenous
when you need to schedule pods performing intensive GPU-based computation only to nodes that provides the required GPU acceleration
if you want to have a say in where a pod should be sheduled, instead of specifying an exact node, you should describe the node requirement and then let Kubernetes select a node that matches those requirement
this can be done through node labels and node label selectors

Using labels for categerizing worker nodes
labels can be attached to any Kubernetes object, including nodes.
when the ops team adds a new node to the cluster, they'll categorize the node by attaching labels specifying the type of hardware the node provides anything else that may come in handy when sheduling pods

$ kubectl label node gke-kubia-xxx-xx-xx gpu=true
$ kubectl get nodes -l gpu=true
$ kubectl get nodes -L gpu

Scheduling pods to specific nodes
now imagine you want to deploy a new pod that needs a GPU to perform its work
to ask the scheduler to only choose among the nodes that provide a GPU, you'll add a node seletor to the pod's YAML
ch03/kubia-gpu.yaml 
kubectl create -f kubia-gpu.yaml

apiVesion: v1
kind: Pod
metadata:
  name: kubia-gpu
spec:
	// node selector tells Kubernetes to deploy this pod only to nodes containing the gpu=true label.
  nodeSelector:
    gpu: "true"
  containers:
    - image: 844348677/kubia 
      name: kubia

you've added a nodeSelector field under the spec section. when you create the pod, the scheduler will only choose among the nodes that contain the gpu=true label

Schedule to on specific node
you could also schedule a pod to an exact node, because each node also has a unique label with the key kubernetes.io/hostname and value set to the actual hostname of the node
but setting the nodeSelector to a specific node by the hostname label may lead to the pod being unschedulable if the node is offline
you shouldn't think in terms of individual nodes. always think about logical groups of nodes that satisfy certain criteria specified through label selectors

Annotating pods
pods and other objects can also contain annotations
annotations are also key-value pairs
but they aren't meant to hold identifying information. they can't be used to group objects the way labels can

Looking up an object's annotations
to see the annotations, you'll need to request the full YAML of the pod or use the kubectl desribe commad
annotations:

Adding and modifying annotations

$ kubectl annotate pod kubia-manual mycompany.com/someannotation="foo bar"
$ kubectl describe pod kubia-manual

Using namespace to group resource

Understanding the need fro namespaces
using multiple namespace allow you to split complex systems with numerous components into smaller distinct groups
spliting up resources into production, development, and QA environment
resource names only need to be unique within namespace
(These aren't the Linux namespace)

Discovering other namespace and their pods

$ kubectl get ns

default ; kube-public ; kube-system

$ kubectl get po --namespace kube-system
$ kubectl get po -n kube-system

besides isolating resources, namespaces are also used for allowing only certain users access to particular resources and even for limiting the amount of computational resources available to individual user

Creating namespace
a namespace is a Kubernetes resources like any other, so you can create it by posting a YAML file to the Kubernetes API server.

Creating a namespace from a YAML file
ch03/custom-namespace.yaml

apiVersion: v1
// this says you're defining a namespace
kind: Namespace
metadata:
	// this is the name of the namespace
  name: custom-namespace

use kubectl to post the file to the Kubernetes API server:

$ kubectl create -f custom-namespace.yaml

Creating a namespace with kubectl create namespace
everything in Kubernetes has a corresponding API object that you can create, read, update, and delete by posting a YAML manifest to the API server

$ kubectl create namespace custom-namespace

Managing objects in other namespaces
to create resourecs in the namespace you've created, either add a namespace:costom-namespace entry to the metadata section, or specify the namespace when creatint the resource with the kubectl create command
$kubectl create -f kubia-manual.yaml -n custom-namespace

Understanding the isolation provided by namespace
they don't provide any kind of isolation of running objects

Stopping and removing pods

Deleting a pod by name

$ kubectl delete po kubia-gpu

by deleting , you're instructing Kubernetes to terminate all the cotainers that are part fo the pod
Kubernetes send a SIGTERM signal to the process and waits a certain number of second (30 by default) for it to shut down gracefully
if it doesn't shut down in time, the process is then killed through SIGKILL
to make sure you process are always shut down gracefully, they need to handle the SIGTERM signal properly

Deleting pods using label selectors

$ kubectl delete po -l creation_method=manual
$ kubectl delete po -l rel=canary

Deleting pods by deleting the whole namespace
you no longer need either the pods in that namespace, or the namespace itself. you can deletes the whole namespace (the pods will be deleted along with the namespace automatically)

$ kubectl delete ns custom-namespace

Deleting all pods in a namespace, while keeping the namespace

$ kubectl get pods
$ kubectl delete po -all
$ kubectl get pods

no matter many times you delete all pods, a new pod called kubia-something will emerge
kubectl run , this doesn't create a pod directly, but instead  creates a ReplicationController, which then creates the pod
to delete the pod, you also need to delete the ReplicationController

Deleting (almost) all resources in a namespace
you can delete the ReplicationController and the pods, as well as all the Services you've created, by deleting all resources in the current namespace with a single command

$ kubectl delete all --all

the first all in the command specifies that you're deleting resources of all types, and the --all option specifies that you're deleting all resource instances instead of specifying them by name
certain resouces ( like Secrets, which we'll introduction in chapter 7) are preserved and need to be deleted explicitly

Summary
	how to decide whether certain containers should be grouped together in a pod or not
	pods can run multiple process and are similar to physical hosts in the contaienr world
	YAML or JSON descriptors can be written and used to create pods and then examined to see the specification of a pod and its current state
	Labels and label selectors should be used to organzie pods and easily perfomr operations on multiple pods at once
	you can use node labels and selectors to schedule pods only to nodes that have certain feature
	Annotations allow attaching larger blobs of data to pods either by people or tools and libraries
	Namespaces can be used to allow different teams to use hte same cluster as though they were using seperate Kubernetes clusters
	how to use the kubectl  explain command to quickly look up the information on any Kubernetes resouces

ch04  Replication and other controller: deploying managed pods
	Keeping pods healthy
	Running multiple instances of the same pod
	Automatically resheduled pods after a node fails 
	scaling pods horizontally
	running system-level pods on each cluster node
	running batch jobs
	scheduling jobs to run periodically or once in the future

pod , you know how to create, supervise, and manange them mannually
you want your deployments to stay up and running automatically and remian healthy without any manual intervention
you create other types of resources, such as ReplicationControllers or Deployments
but if the whole node fails, the pods on the node are lost and will not be replaced with new ones, unless those pods are managed by the previously mentioned ReplicationController or similar

Keeping pods healthy
the ability to give it a list of contaienrs and let it keep those containers running somewhere in the cluster
as soon as a pod is scheduled to a node, the Kubelet on that node will run its containers and, from then on, keep them running as long as the pod exists
if the container's main process crashes, the Kubelet will restart the container

Introducing liveness probes
Kubernetes can check if a container is still alive through liveness probes
you can specify a liveness probe for each container in the pod's specification 
Kubernetes will periodically execute the probe and restart the container if the probe fails

Kubernetes can probe a container using one of the three mechanisms
	an HTTP GET probe performs an HTTP GET request on the container's IP address, a port an path you specify. if the probe receives a response, and the response code doesn't represent an error(in other words, if the HTTP response code is 2xx or 3xx), the probe is considered successful. if the server returns an error response code or if it doesn't respond at all, the probe is considered a failure and the container will be restarted as a result
	A TCP Socket probe tries to open a TCP connetion to the specified port of the container. if the connection is established successfully, the probe is successful. Otherwise, the container is restarted
	an Exec probe executes an arbitrary command inside the container and checks the command's exit status code. if the status code is 0, the probe is successful. all other codes are considered failure

Creating an http-based liveness probe
create a new pod that includes an HTTP GET liveness probe
ch04/kubia-probe.yaml

apiVersion: v1
kind: pod
metadata:
  name: kubia-liveness
spec:
  containers:
  	// this is the image containing the (somewhat) broken app
    - image: luksa/kubia-unhealthy
      name: kubia
      // a liveness probe that will perform an HTTP GET
      livenessProbe:
        httpGet:
        	// the path to request in the HTTP request
          path: /
          // the network port the  probe should connect to 
          port: 8080

the pod descriptor defines an httpGet liveness probe, which tells Kubernetes to periodically perform HTTP GET request on path / on port 8080 to determine if the container is still healthy

Seeing a liveness probe in action

$ kubectl get po kubia-liveness

the RESTARTS column shows that the pod's container has been restarted once

Obtaining the application log of a crash container

$ kubectl logs mypod --previous 

$ kubectl describe po kubia-liveness

the exit code was 137, which has special meaning-it denotes that the process was terminated by an external signal. 
the number 137 is a sum of two numbers:128+x, where x is the signal number sent to the process that caused it to terminate. in the example, x equals 9, which is the number of the SIGKILL signal, meaning the process was killed forcibly

Configuring additional properties of the liveness probe

Liveness: http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 #failure=3

beside the liveness probe options you specified explicitly, you can also see additional properties, such as delay, timeout, period, and so on
the delay=0s part shows that the probing begins immediately after the container is started
the timeout is set to only 1 second, so the container must return a response in 1 second or the probe is counted as failed
the container is probed every 10 seconds (period=10s) and the container is restarted after the probe fails three consecutive times (#failure=3)
ch04/kubia-liveness-probe-initial-delay.yaml

      livenessProbe:
        httpGet:
          path: /
          port: 8080
          // kubernetes will wait 15 seconds before executing the first probe
        initialDelaySeconds: 15

Always remember to set an initial delay to account for your app's startup time
exit code is 128+9 (SIGKILL). likewise, exit code 143 corresponds to 128+15(SIGTERM)

Creating effective liveness probes

What a liveness probe should check
but for a better liveness chech, you'd configure the probe to perform request on a specific URL path(/heath, for example) and have the app perform an internal status check of all the vital components running inside the app to ensure none of them has died or is unresponsive
be sure to check only the internals of the app and nothing influenced by an external factor
a frontend web server's liveness probe shouldn't return cause a failure when the server can't connect to the backend database.
if the underlying cause is in the database itself, restarting the web server container will not fix the problem

Keeping probes light
Liveness probes shouldn't use too many computational resources and shouldn't take too long to complete
the probe's CPU time is counted in the container's CPU time quota, so having a heavyweight liveness probe will reduce the CPU time available to the main application processes

Don't bother implementing retry loops in your probes

Liveness probe wrap-up
this job is performed by the Kubelet on the node hosting the pod - the Kubernetes Control Plane components running on the master(s) have no part in the process
but if the node itself crashes, it's the Control Plane that must create replacements for all the pods that went down with the code

Introducing ReplicationgControllers
a ReplicationController is a Kubernetes resource that ensures its pod are always kept running 

the operation of a ReplicationController
a ReplicationController constantly monitors the list of running pods and makes sure the actual number of pods of a "type" always matches the desired number
if too few such pods are running, it creates new replicas from a pod templete. if too many such pods are running , it removes the excess replicas
ReplicationController operate on sets of pods that match a certain label selector

Introducing the controller's reconciliation loop




