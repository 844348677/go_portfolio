ch03 Pods:running containers in Kubernetes
	creating, running, and stopping pods
	organizing pods and other resources with labels
	performing an operation on all pods with a specific label
	using namespaces to split pods into non-overlapping groups
	scheduling pods onto specific types of worker nodes

Introducing pods
when a pod does contain multiple containers, all of them are always run on a single worker node, it never spans multiple worker nodes

Understanding why we need pods

Understanding why multiple containers are better than one container running multiple precess
containers are designed to run only a single process per container (unless the process itself spawn)

Understanding pods
you need another higher-level construct that will allow you to bind containers together and manage them as a single unit
almost the same environment as if ther were all running in a single container, while keeping them somewhat isolated
you can take advangtage of all the features containers provide, while at the same time giving the processes the illusion of running together

Understanding the partial isolation between containers of the same pod
you want to isolate groups of containers instead of individual ones
Kubernetes achieves this by configuring Docker to have all container of a pod share the same set of Linux namespaces instead of each container having its own set
Because all containers of a pod run under the same Network and UTS namespaces(Linux namespace here), they all share the same hostname and network interfaces
similarly, all container of a pod run under the same IPC namespace and can communicate through IPC (Inter-Process Communication), and can communication through IPC. (PID namespace)
when containers of the same pod use seperate PID namespaces, you only see the contaienr's own processes when running ps aux in the container
But when ti comes to the filesystem, things are a little different.
because most of the container's filesystem comes from the container image, by default, the filesystem of each container is fully isolated from other containers
using a Kubernetes concept called a Volume

Understanding how containers share the same IP and PORT space
because containers in a pod run in the same Network namespace, they share the same IP address and port space
ports conflicts

Introducing the flat inter-pod network
every pod can access every other pod at the other pod's IP address
communication between pods is always simple
it doesn't matter if two pod are sheduled onto a single or onto different worker nodes
containers inside those pods can communicate with each other across the flat NAT-less (Network Address Translation) network, much like computers on a local area network (LAN)
pods are logical hosts and behave much like physical hosts or VMs in the non-container world

Organizing containers across pods properly
you should think of pods as seperate mahines
unlike the old days, when we used to cram all sorts of apps onto the same hosts
you should organize app into multiple pods, where each one contains only tightly related components or process

Splitting multi-tier apps into multiple pods

Splitting into multiple pods to enable individual scaling
Kubernetes can't horizontally scale individual containers; instead, it scales whole pods

Understanding when to use multiple containers in a pod
wehn the application consists of one process and one or more complementary process
Pods should contain tightly coupled containers, usually a main container and contianers that support the main one
for example, the main container in a pod could be a web server that serves files from a certain file directory, while an additional container (a sidecar container) periodically downloads content from an external source and stores it in the web server's directory
(use a Kubernetes Volume that you mount into both containers)

Deciding when to use multiple containers in a pod
recap how container should be grouped into pods
	Do they need to be run together or can they run on different hosts
	Do they represent a single whole or are they independent components
	Must they be scaled together or individually
a container shouldn't run multiple process. a pod shouldn't contain multiple containers if they don't need to run on the same machine

Creating pods from YAML or JSON descriptors
pods an other kubernetes resources are usually created by posting a JSON or YAML manifest to the Kubernetes REST API endpoint

Examining a YAML descriptor of an existing pod
you'll use the kubectl get command with the -o yaml option to get the whole YAML definition of the pod

$ kubectl get po kubia-zxzij -o yaml

Introducing the main parts of a pod definition
first, there's the Kubernetes API version used in the YAML and the type of resource the YAML is describing
then, three important section are found in almost all Kubernets resouces
	Metadata includes the name, namespace, labels, and other infromation about the pod
	Spec contains the actual description of the pod's contents, such as the pod's containers, volumes, and other data
	Status contains the current information about the running pod, such as what condition the pod is in, the description and status of each container, and the pod's inteernal IP and other basic info
the status part contains read-only runtime data that shows the state of the resource at a given moment.
ch03/kubia-manual.yaml

Creating a simple YAML descriptor for a pod
// Descriptor conforms to version v1 of Kubernetes API
apiVersion: V1
// you're describing a pod
kind: Pod
metadata:
	// the name of the pod
  name: kubia-manual
spec:
  containers:
  	// Container image to create the container from 
    - image: 844348677/kubia
    	// name of the contaienr
       name: kubia
       // the port the app is listening on
       ports:
       - containerPort: 8080
          protocol: TCP

Specifying container ports
it makes sense to define the ports explicitly so that everyone using your cluster can quickly see what ports each pod exposes
explicitly defining ports also allows you to assign a name to each port

$ kubectl explain pods
$ kubectl explain pod.spec

Using kubectl create to create the pod
to create the pod from your YAML file, use the kubectl create command

$ kubectl create -f kubia-manual.yaml

the kubectl create -f command is used fro creating any resource (not only pods) from a YAML or JSON file

Retrieving the whole definition of a running pod
after creating the pod, you can ask Kubernetes for the full YAML of the pod

$ kubectl get po kubia-manual -o yaml
$ kubectl get po kubia-manual -o json

Seeing your newly created pod in the list of pods

$ kubectl get pods

Viewing application logs
containerized application usually log to the standard output and standard error stream instead of writing their logs to files
the container runtime (Docker in your case) redirects those stream to files and allow you to get the container's log by running

$ docker logs <container id>

Retrieving a pod's log with kubectl logs

$ kubectl logs kubia-manual

Specifying the container name when getting logs fo a multi-container pod

$ kubectl logs kubia-manul -c kubia

centralized, cluster-wide logging, which stores all the logs into a central store

Sending requests to the pod
the kubectl expose command to create a service to gain access to the pod externally
other ways of connecting to a pod for testing and debugging purposes
one of them is through port forwarding

Forwarding a local network port to a port in the pod
when you want to talk to a specific pod without going through a service (for debugging or other reasonss), Kubernetes allows you to configure port forwarding to the pod
local port 8888 to port 8080 of your kubia-manual pod

$ kubectl port-forward kubia-manual 8888:8080

Connecting to the pod trough the port forwarder

$ curl localhost:8888

using port forwarding like this is an effective way to test an individual pod

Organizing pods with labels
as the number of pods increases, the need for categorizing them into subsets becomes more and more evident
microservices architectures
those components will porbably be replicated(multiple copies of the same componen will be deployed) and multiple versions or releases (stable, beta, canary, and so on) will run concurrently
you'll want to operate on every pod belonging to a certain group with a single action instead of having to perform the action for each pod  individually
organizing pods and all other Kubernetes objects is done through labels

Introducing labels
labels are simple, yet incredibly powerful, Kubernetes feature for organizing not only pods, but all other Kubernetes resources
a label is an arbitrary key-value pair you attach to a resource, which is then utilized when selecting resources using label selectors ( resources are filtered based on whether they include the label specified in the selector)
a resource can have more than on label, as long as the keys of the labels are unique within that resource.
you usually attach labels to resources when you create them, but you can also add additional labels or even modify the values of existing labels later without having to recreate the resource
by adding labels to those pods, you get a much-better-organized system that everyone can easily make sense of
	app, which specifies which app, component, or microservice the pod belongs to
	rel, which shows whether the application running in the pod is a stable, beta, or a canary release
by adding these two labels, you've essentially organized your pods into two dimensions (horizontally by app and vertically by release)

Specifying labels when creating a pod
labels in action by creating a new pod with two labels
ch03/kubia-manual-with-labels.yaml

apiVersion: v1
kind: Pod
metadata:
  name: kubia-manual-v2
  // two labels are attached to the pod
  labels:
    creation_method: manual 
    env: prod
spec:
  containers:
    - image: 844348677/kubia
       name: kubia
       ports:
       - containerPort: 8080
          protocol: TCP

the labels creatinon_method=manual and env=prod

$ kubectl create -f kubia-manul-with-labels.yaml
$ kubectl get po --show-labels

instead of listing all labels, if you're only interested in certain labels, you can specify them with the -L switch and have each displayed in its own column

$ kubectl get po -L creation_method,env

Modify labels of existing pods
labels can also be added to and modified on existing pods

$ kubectl label po kubia-manual creation_method=manula
$ kubectl label po kubia-manula-v2 env=debug --overwrite

you need to use the --overwrite option when changing existing labels

$ kubectl get po -L creation_method,env

Listing subsets of pods through label selectors
label selectors allow you to select a subset of pods tagged with certain labels and perform an operation on those pods
a label selector is a criterion, which filters resources based on whether they include a certian label with a certain value
	contains (or doesn't contain) a label with a certain key
	contains a label with a certain key and value
	contains a label with a certain key, but with a value not equal to the one you specify

Listing pods using a label selector

$ kubectl get po -l creation_method=manual
$ kubectl get po -l env
$ kubectl get po -l '!env'

	creation_method!=manual to select pods with the creation_method label  with any value other than manual
	env in (prod,devel) to select pods with the env label set to either prod or devel
	env notion (prod,devel) to select pods with the env label set to any value other than prod or devel

Using multiple condition in a label selector
a selector can also include multiple comma-separated criteria
label selectors aren't usefor only fro listing pods, but also for performing actions on a subset of all pods

Using labels and selectors to constrain pod scheduling
when your hardware infrastructure isn't homogenous
when you need to schedule pods performing intensive GPU-based computation only to nodes that provides the required GPU acceleration
if you want to have a say in where a pod should be sheduled, instead of specifying an exact node, you should describe the node requirement and then let Kubernetes select a node that matches those requirement
this can be done through node labels and node label selectors

Using labels for categerizing worker nodes
labels can be attached to any Kubernetes object, including nodes.
when the ops team adds a new node to the cluster, they'll categorize the node by attaching labels specifying the type of hardware the node provides anything else that may come in handy when sheduling pods

$ kubectl label node gke-kubia-xxx-xx-xx gpu=true
$ kubectl get nodes -l gpu=true
$ kubectl get nodes -L gpu

Scheduling pods to specific nodes
now imagine you want to deploy a new pod that needs a GPU to perform its work
to ask the scheduler to only choose among the nodes that provide a GPU, you'll add a node seletor to the pod's YAML
ch03/kubia-gpu.yaml 
kubectl create -f kubia-gpu.yaml

apiVesion: v1
kind: Pod
metadata:
  name: kubia-gpu
spec:
	// node selector tells Kubernetes to deploy this pod only to nodes containing the gpu=true label.
  nodeSelector:
    gpu: "true"
  containers:
    - image: 844348677/kubia 
      name: kubia

you've added a nodeSelector field under the spec section. when you create the pod, the scheduler will only choose among the nodes that contain the gpu=true label

Schedule to on specific node
you could also schedule a pod to an exact node, because each node also has a unique label with the key kubernetes.io/hostname and value set to the actual hostname of the node
but setting the nodeSelector to a specific node by the hostname label may lead to the pod being unschedulable if the node is offline
you shouldn't think in terms of individual nodes. always think about logical groups of nodes that satisfy certain criteria specified through label selectors

Annotating pods
pods and other objects can also contain annotations
annotations are also key-value pairs
but they aren't meant to hold identifying information. they can't be used to group objects the way labels can

Looking up an object's annotations
to see the annotations, you'll need to request the full YAML of the pod or use the kubectl desribe commad
annotations:

Adding and modifying annotations

$ kubectl annotate pod kubia-manual mycompany.com/someannotation="foo bar"
$ kubectl describe pod kubia-manual

Using namespace to group resource

Understanding the need fro namespaces
using multiple namespace allow you to split complex systems with numerous components into smaller distinct groups
spliting up resources into production, development, and QA environment
resource names only need to be unique within namespace
(These aren't the Linux namespace)

Discovering other namespace and their pods

$ kubectl get ns

default ; kube-public ; kube-system

$ kubectl get po --namespace kube-system
$ kubectl get po -n kube-system

besides isolating resources, namespaces are also used for allowing only certain users access to particular resources and even for limiting the amount of computational resources available to individual user

Creating namespace
a namespace is a Kubernetes resources like any other, so you can create it by posting a YAML file to the Kubernetes API server.

Creating a namespace from a YAML file
ch03/custom-namespace.yaml

apiVersion: v1
// this says you're defining a namespace
kind: Namespace
metadata:
	// this is the name of the namespace
  name: custom-namespace

use kubectl to post the file to the Kubernetes API server:

$ kubectl create -f custom-namespace.yaml

Creating a namespace with kubectl create namespace
everything in Kubernetes has a corresponding API object that you can create, read, update, and delete by posting a YAML manifest to the API server

$ kubectl create namespace custom-namespace

Managing objects in other namespaces








