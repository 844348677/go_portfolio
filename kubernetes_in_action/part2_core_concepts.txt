ch03 Pods:running containers in Kubernetes
	creating, running, and stopping pods
	organizing pods and other resources with labels
	performing an operation on all pods with a specific label
	using namespaces to split pods into non-overlapping groups
	scheduling pods onto specific types of worker nodes

Introducing pods
when a pod does contain multiple containers, all of them are always run on a single worker node, it never spans multiple worker nodes

Understanding why we need pods

Understanding why multiple containers are better than one container running multiple precess
containers are designed to run only a single process per container (unless the process itself spawn)

Understanding pods
you need another higher-level construct that will allow you to bind containers together and manage them as a single unit
almost the same environment as if ther were all running in a single container, while keeping them somewhat isolated
you can take advangtage of all the features containers provide, while at the same time giving the processes the illusion of running together

Understanding the partial isolation between containers of the same pod
you want to isolate groups of containers instead of individual ones
Kubernetes achieves this by configuring Docker to have all container of a pod share the same set of Linux namespaces instead of each container having its own set
Because all containers of a pod run under the same Network and UTS namespaces(Linux namespace here), they all share the same hostname and network interfaces
similarly, all container of a pod run under the same IPC namespace and can communicate through IPC (Inter-Process Communication), and can communication through IPC. (PID namespace)
when containers of the same pod use seperate PID namespaces, you only see the contaienr's own processes when running ps aux in the container
But when ti comes to the filesystem, things are a little different.
because most of the container's filesystem comes from the container image, by default, the filesystem of each container is fully isolated from other containers
using a Kubernetes concept called a Volume

Understanding how containers share the same IP and PORT space
because containers in a pod run in the same Network namespace, they share the same IP address and port space
ports conflicts

Introducing the flat inter-pod network
every pod can access every other pod at the other pod's IP address
communication between pods is always simple
it doesn't matter if two pod are sheduled onto a single or onto different worker nodes
containers inside those pods can communicate with each other across the flat NAT-less (Network Address Translation) network, much like computers on a local area network (LAN)
pods are logical hosts and behave much like physical hosts or VMs in the non-container world

Organizing containers across pods properly
you should think of pods as seperate mahines
unlike the old days, when we used to cram all sorts of apps onto the same hosts
you should organize app into multiple pods, where each one contains only tightly related components or process

Splitting multi-tier apps into multiple pods

Splitting into multiple pods to enable individual scaling
Kubernetes can't horizontally scale individual containers; instead, it scales whole pods

Understanding when to use multiple containers in a pod
wehn the application consists of one process and one or more complementary process
Pods should contain tightly coupled containers, usually a main container and contianers that support the main one
for example, the main container in a pod could be a web server that serves files from a certain file directory, while an additional container (a sidecar container) periodically downloads content from an external source and stores it in the web server's directory
(use a Kubernetes Volume that you mount into both containers)

Deciding when to use multiple containers in a pod
recap how container should be grouped into pods
	Do they need to be run together or can they run on different hosts
	Do they represent a single whole or are they independent components
	Must they be scaled together or individually
a container shouldn't run multiple process. a pod shouldn't contain multiple containers if they don't need to run on the same machine

Creating pods from YAML or JSON descriptors
pods an other kubernetes resources are usually created by posting a JSON or YAML manifest to the Kubernetes REST API endpoint

Examining a YAML descriptor of an existing pod
you'll use the kubectl get command with the -o yaml option to get the whole YAML definition of the pod

$ kubectl get po kubia-zxzij -o yaml

Introducing the main parts of a pod definition
first, there's the Kubernetes API version used in the YAML and the type of resource the YAML is describing
then, three important section are found in almost all Kubernets resouces
	Metadata includes the name, namespace, labels, and other infromation about the pod
	Spec contains the actual description of the pod's contents, such as the pod's containers, volumes, and other data
	Status contains the current information about the running pod, such as what condition the pod is in, the description and status of each container, and the pod's inteernal IP and other basic info
the status part contains read-only runtime data that shows the state of the resource at a given moment.
ch03/kubia-manual.yaml

Creating a simple YAML descriptor for a pod
// Descriptor conforms to version v1 of Kubernetes API
apiVersion: V1
// you're describing a pod
kind: Pod
metadata:
	// the name of the pod
  name: kubia-manual
spec:
  containers:
  	// Container image to create the container from 
    - image: 844348677/kubia
    	// name of the contaienr
       name: kubia
       // the port the app is listening on
       ports:
       - containerPort: 8080
          protocol: TCP

Specifying container ports
it makes sense to define the ports explicitly so that everyone using your cluster can quickly see what ports each pod exposes
explicitly defining ports also allows you to assign a name to each port

$ kubectl explain pods
$ kubectl explain pod.spec

Using kubectl create to create the pod
to create the pod from your YAML file, use the kubectl create command

$ kubectl create -f kubia-manual.yaml

the kubectl create -f command is used fro creating any resource (not only pods) from a YAML or JSON file

Retrieving the whole definition of a running pod
after creating the pod, you can ask Kubernetes for the full YAML of the pod

$ kubectl get po kubia-manual -o yaml
$ kubectl get po kubia-manual -o json

Seeing your newly created pod in the list of pods

$ kubectl get pods

Viewing application logs
containerized application usually log to the standard output and standard error stream instead of writing their logs to files
the container runtime (Docker in your case) redirects those stream to files and allow you to get the container's log by running

$ docker logs <container id>

Retrieving a pod's log with kubectl logs

$ kubectl logs kubia-manual

Specifying the container name when getting logs fo a multi-container pod

$ kubectl logs kubia-manul -c kubia

centralized, cluster-wide logging, which stores all the logs into a central store

Sending requests to the pod
the kubectl expose command to create a service to gain access to the pod externally
other ways of connecting to a pod for testing and debugging purposes
one of them is through port forwarding

Forwarding a local network port to a port in the pod
when you want to talk to a specific pod without going through a service (for debugging or other reasonss), Kubernetes allows you to configure port forwarding to the pod
local port 8888 to port 8080 of your kubia-manual pod

$ kubectl port-forward kubia-manual 8888:8080

Connecting to the pod trough the port forwarder

$ curl localhost:8888

using port forwarding like this is an effective way to test an individual pod

Organizing pods with labels
as the number of pods increases, the need for categorizing them into subsets becomes more and more evident
microservices architectures
those components will porbably be replicated(multiple copies of the same componen will be deployed) and multiple versions or releases (stable, beta, canary, and so on) will run concurrently
you'll want to operate on every pod belonging to a certain group with a single action instead of having to perform the action for each pod  individually
organizing pods and all other Kubernetes objects is done through labels

Introducing labels
labels are simple, yet incredibly powerful, Kubernetes feature for organizing not only pods, but all other Kubernetes resources
a label is an arbitrary key-value pair you attach to a resource, which is then utilized when selecting resources using label selectors ( resources are filtered based on whether they include the label specified in the selector)
a resource can have more than on label, as long as the keys of the labels are unique within that resource.
you usually attach labels to resources when you create them, but you can also add additional labels or even modify the values of existing labels later without having to recreate the resource
by adding labels to those pods, you get a much-better-organized system that everyone can easily make sense of
	app, which specifies which app, component, or microservice the pod belongs to
	rel, which shows whether the application running in the pod is a stable, beta, or a canary release
by adding these two labels, you've essentially organized your pods into two dimensions (horizontally by app and vertically by release)

Specifying labels when creating a pod
labels in action by creating a new pod with two labels
ch03/kubia-manual-with-labels.yaml

apiVersion: v1
kind: Pod
metadata:
  name: kubia-manual-v2
  // two labels are attached to the pod
  labels:
    creation_method: manual 
    env: prod
spec:
  containers:
    - image: 844348677/kubia
       name: kubia
       ports:
       - containerPort: 8080
          protocol: TCP

the labels creatinon_method=manual and env=prod

$ kubectl create -f kubia-manul-with-labels.yaml
$ kubectl get po --show-labels

instead of listing all labels, if you're only interested in certain labels, you can specify them with the -L switch and have each displayed in its own column

$ kubectl get po -L creation_method,env

Modify labels of existing pods
labels can also be added to and modified on existing pods

$ kubectl label po kubia-manual creation_method=manula
$ kubectl label po kubia-manula-v2 env=debug --overwrite

you need to use the --overwrite option when changing existing labels

$ kubectl get po -L creation_method,env

Listing subsets of pods through label selectors
label selectors allow you to select a subset of pods tagged with certain labels and perform an operation on those pods
a label selector is a criterion, which filters resources based on whether they include a certian label with a certain value
	contains (or doesn't contain) a label with a certain key
	contains a label with a certain key and value
	contains a label with a certain key, but with a value not equal to the one you specify

Listing pods using a label selector

$ kubectl get po -l creation_method=manual
$ kubectl get po -l env
$ kubectl get po -l '!env'

	creation_method!=manual to select pods with the creation_method label  with any value other than manual
	env in (prod,devel) to select pods with the env label set to either prod or devel
	env notion (prod,devel) to select pods with the env label set to any value other than prod or devel

Using multiple condition in a label selector
a selector can also include multiple comma-separated criteria
label selectors aren't usefor only fro listing pods, but also for performing actions on a subset of all pods

Using labels and selectors to constrain pod scheduling
when your hardware infrastructure isn't homogenous
when you need to schedule pods performing intensive GPU-based computation only to nodes that provides the required GPU acceleration
if you want to have a say in where a pod should be sheduled, instead of specifying an exact node, you should describe the node requirement and then let Kubernetes select a node that matches those requirement
this can be done through node labels and node label selectors

Using labels for categerizing worker nodes
labels can be attached to any Kubernetes object, including nodes.
when the ops team adds a new node to the cluster, they'll categorize the node by attaching labels specifying the type of hardware the node provides anything else that may come in handy when sheduling pods

$ kubectl label node gke-kubia-xxx-xx-xx gpu=true
$ kubectl get nodes -l gpu=true
$ kubectl get nodes -L gpu

Scheduling pods to specific nodes
now imagine you want to deploy a new pod that needs a GPU to perform its work
to ask the scheduler to only choose among the nodes that provide a GPU, you'll add a node seletor to the pod's YAML
ch03/kubia-gpu.yaml 
kubectl create -f kubia-gpu.yaml

apiVesion: v1
kind: Pod
metadata:
  name: kubia-gpu
spec:
	// node selector tells Kubernetes to deploy this pod only to nodes containing the gpu=true label.
  nodeSelector:
    gpu: "true"
  containers:
    - image: 844348677/kubia 
      name: kubia

you've added a nodeSelector field under the spec section. when you create the pod, the scheduler will only choose among the nodes that contain the gpu=true label

Schedule to on specific node
you could also schedule a pod to an exact node, because each node also has a unique label with the key kubernetes.io/hostname and value set to the actual hostname of the node
but setting the nodeSelector to a specific node by the hostname label may lead to the pod being unschedulable if the node is offline
you shouldn't think in terms of individual nodes. always think about logical groups of nodes that satisfy certain criteria specified through label selectors

Annotating pods
pods and other objects can also contain annotations
annotations are also key-value pairs
but they aren't meant to hold identifying information. they can't be used to group objects the way labels can

Looking up an object's annotations
to see the annotations, you'll need to request the full YAML of the pod or use the kubectl desribe commad
annotations:

Adding and modifying annotations

$ kubectl annotate pod kubia-manual mycompany.com/someannotation="foo bar"
$ kubectl describe pod kubia-manual

Using namespace to group resource

Understanding the need fro namespaces
using multiple namespace allow you to split complex systems with numerous components into smaller distinct groups
spliting up resources into production, development, and QA environment
resource names only need to be unique within namespace
(These aren't the Linux namespace)

Discovering other namespace and their pods

$ kubectl get ns

default ; kube-public ; kube-system

$ kubectl get po --namespace kube-system
$ kubectl get po -n kube-system

besides isolating resources, namespaces are also used for allowing only certain users access to particular resources and even for limiting the amount of computational resources available to individual user

Creating namespace
a namespace is a Kubernetes resources like any other, so you can create it by posting a YAML file to the Kubernetes API server.

Creating a namespace from a YAML file
ch03/custom-namespace.yaml

apiVersion: v1
// this says you're defining a namespace
kind: Namespace
metadata:
	// this is the name of the namespace
  name: custom-namespace

use kubectl to post the file to the Kubernetes API server:

$ kubectl create -f custom-namespace.yaml

Creating a namespace with kubectl create namespace
everything in Kubernetes has a corresponding API object that you can create, read, update, and delete by posting a YAML manifest to the API server

$ kubectl create namespace custom-namespace

Managing objects in other namespaces
to create resourecs in the namespace you've created, either add a namespace:costom-namespace entry to the metadata section, or specify the namespace when creatint the resource with the kubectl create command
$kubectl create -f kubia-manual.yaml -n custom-namespace

Understanding the isolation provided by namespace
they don't provide any kind of isolation of running objects

Stopping and removing pods

Deleting a pod by name

$ kubectl delete po kubia-gpu

by deleting , you're instructing Kubernetes to terminate all the cotainers that are part fo the pod
Kubernetes send a SIGTERM signal to the process and waits a certain number of second (30 by default) for it to shut down gracefully
if it doesn't shut down in time, the process is then killed through SIGKILL
to make sure you process are always shut down gracefully, they need to handle the SIGTERM signal properly

Deleting pods using label selectors

$ kubectl delete po -l creation_method=manual
$ kubectl delete po -l rel=canary

Deleting pods by deleting the whole namespace
you no longer need either the pods in that namespace, or the namespace itself. you can deletes the whole namespace (the pods will be deleted along with the namespace automatically)

$ kubectl delete ns custom-namespace

Deleting all pods in a namespace, while keeping the namespace

$ kubectl get pods
$ kubectl delete po -all
$ kubectl get pods

no matter many times you delete all pods, a new pod called kubia-something will emerge
kubectl run , this doesn't create a pod directly, but instead  creates a ReplicationController, which then creates the pod
to delete the pod, you also need to delete the ReplicationController

Deleting (almost) all resources in a namespace
you can delete the ReplicationController and the pods, as well as all the Services you've created, by deleting all resources in the current namespace with a single command

$ kubectl delete all --all

the first all in the command specifies that you're deleting resources of all types, and the --all option specifies that you're deleting all resource instances instead of specifying them by name
certain resouces ( like Secrets, which we'll introduction in chapter 7) are preserved and need to be deleted explicitly

Summary
	how to decide whether certain containers should be grouped together in a pod or not
	pods can run multiple process and are similar to physical hosts in the contaienr world
	YAML or JSON descriptors can be written and used to create pods and then examined to see the specification of a pod and its current state
	Labels and label selectors should be used to organzie pods and easily perfomr operations on multiple pods at once
	you can use node labels and selectors to schedule pods only to nodes that have certain feature
	Annotations allow attaching larger blobs of data to pods either by people or tools and libraries
	Namespaces can be used to allow different teams to use hte same cluster as though they were using seperate Kubernetes clusters
	how to use the kubectl  explain command to quickly look up the information on any Kubernetes resouces

ch04  Replication and other controller: deploying managed pods
	Keeping pods healthy
	Running multiple instances of the same pod
	Automatically resheduled pods after a node fails 
	scaling pods horizontally
	running system-level pods on each cluster node
	running batch jobs
	scheduling jobs to run periodically or once in the future

pod , you know how to create, supervise, and manange them mannually
you want your deployments to stay up and running automatically and remian healthy without any manual intervention
you create other types of resources, such as ReplicationControllers or Deployments
but if the whole node fails, the pods on the node are lost and will not be replaced with new ones, unless those pods are managed by the previously mentioned ReplicationController or similar

Keeping pods healthy
the ability to give it a list of contaienrs and let it keep those containers running somewhere in the cluster
as soon as a pod is scheduled to a node, the Kubelet on that node will run its containers and, from then on, keep them running as long as the pod exists
if the container's main process crashes, the Kubelet will restart the container

Introducing liveness probes
Kubernetes can check if a container is still alive through liveness probes
you can specify a liveness probe for each container in the pod's specification 
Kubernetes will periodically execute the probe and restart the container if the probe fails

Kubernetes can probe a container using one of the three mechanisms
	an HTTP GET probe performs an HTTP GET request on the container's IP address, a port an path you specify. if the probe receives a response, and the response code doesn't represent an error(in other words, if the HTTP response code is 2xx or 3xx), the probe is considered successful. if the server returns an error response code or if it doesn't respond at all, the probe is considered a failure and the container will be restarted as a result
	A TCP Socket probe tries to open a TCP connetion to the specified port of the container. if the connection is established successfully, the probe is successful. Otherwise, the container is restarted
	an Exec probe executes an arbitrary command inside the container and checks the command's exit status code. if the status code is 0, the probe is successful. all other codes are considered failure

Creating an http-based liveness probe
create a new pod that includes an HTTP GET liveness probe
ch04/kubia-probe.yaml

apiVersion: v1
kind: pod
metadata:
  name: kubia-liveness
spec:
  containers:
  	// this is the image containing the (somewhat) broken app
    - image: luksa/kubia-unhealthy
      name: kubia
      // a liveness probe that will perform an HTTP GET
      livenessProbe:
        httpGet:
        	// the path to request in the HTTP request
          path: /
          // the network port the  probe should connect to 
          port: 8080

the pod descriptor defines an httpGet liveness probe, which tells Kubernetes to periodically perform HTTP GET request on path / on port 8080 to determine if the container is still healthy

Seeing a liveness probe in action

$ kubectl get po kubia-liveness

the RESTARTS column shows that the pod's container has been restarted once

Obtaining the application log of a crash container

$ kubectl logs mypod --previous 

$ kubectl describe po kubia-liveness

the exit code was 137, which has special meaning-it denotes that the process was terminated by an external signal. 
the number 137 is a sum of two numbers:128+x, where x is the signal number sent to the process that caused it to terminate. in the example, x equals 9, which is the number of the SIGKILL signal, meaning the process was killed forcibly

Configuring additional properties of the liveness probe

Liveness: http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 #failure=3

beside the liveness probe options you specified explicitly, you can also see additional properties, such as delay, timeout, period, and so on
the delay=0s part shows that the probing begins immediately after the container is started
the timeout is set to only 1 second, so the container must return a response in 1 second or the probe is counted as failed
the container is probed every 10 seconds (period=10s) and the container is restarted after the probe fails three consecutive times (#failure=3)
ch04/kubia-liveness-probe-initial-delay.yaml

      livenessProbe:
        httpGet:
          path: /
          port: 8080
          // kubernetes will wait 15 seconds before executing the first probe
        initialDelaySeconds: 15

Always remember to set an initial delay to account for your app's startup time
exit code is 128+9 (SIGKILL). likewise, exit code 143 corresponds to 128+15(SIGTERM)

Creating effective liveness probes

What a liveness probe should check
but for a better liveness chech, you'd configure the probe to perform request on a specific URL path(/heath, for example) and have the app perform an internal status check of all the vital components running inside the app to ensure none of them has died or is unresponsive
be sure to check only the internals of the app and nothing influenced by an external factor
a frontend web server's liveness probe shouldn't return cause a failure when the server can't connect to the backend database.
if the underlying cause is in the database itself, restarting the web server container will not fix the problem

Keeping probes light
Liveness probes shouldn't use too many computational resources and shouldn't take too long to complete
the probe's CPU time is counted in the container's CPU time quota, so having a heavyweight liveness probe will reduce the CPU time available to the main application processes

Don't bother implementing retry loops in your probes

Liveness probe wrap-up
this job is performed by the Kubelet on the node hosting the pod - the Kubernetes Control Plane components running on the master(s) have no part in the process
but if the node itself crashes, it's the Control Plane that must create replacements for all the pods that went down with the code

Introducing ReplicationgControllers
a ReplicationController is a Kubernetes resource that ensures its pod are always kept running 

the operation of a ReplicationController
a ReplicationController constantly monitors the list of running pods and makes sure the actual number of pods of a "type" always matches the desired number
if too few such pods are running, it creates new replicas from a pod templete. if too many such pods are running , it removes the excess replicas
ReplicationController operate on sets of pods that match a certain label selector

Introducing the controller's reconciliation loop
a ReplicationController's job is to make sure that an exact number of pod always matches its label selector

Understanding the three parts of a ReplicatonController
a ReplicationController has three essential parts
A label selector, which determines what pods are in the ReplicationController's scope
A replica count, which specifies the desired number of pods that should be running
A pod template, which is used when creating new pod replicas

ReplicationControler: kubiao {Pod selector:app=kubia ; Replica:3 ; Pod template:{Pod(app:kubia)}}
a ReplicationController's replica count, the label selector, and even the pod template can all be modified at any time, but only changes to the replica count affect existing pods

Understanding the effect of changing teh controller's label selector or pod template
changes to the label selector and the pod template have no effect on existing pods
changing the label selector makes teh existing pod fall out of the scope of the ReplicationController

Understing teh benefit of using a ReplicationController
a ReplicationController, although an incredibly simple concept, provides or enable the following powerful features
	it makes sure a pod(or multiple pod replicas) is always running by starting a new pod when an existing one goes missing
	when a cluster node fails, it creates replacement replicas for all the pods that were running on the failed node ( those that were under the ReplicationController's control)
	it enable easy horizontal scaling of pods - both manual and automatic
a pod instance is never relocated to another node. instead, the ReplicationController creates completely new pod instance that has  no relation to the instance it's replacing

Creating a ReplicationController
how to create a ReplicationController , how it keeps your pods running
ch04/kubia-rc.yaml

apiVersion: v1
// this manifest defines a ReplicationController (RC)
kind: ReplicationController
metadata:
// the name of this ReplicationController
  name: kubia
spec:
	// the desired number of pod instances
  replicas: 3
  // the pod selector determining what pods the RC is operating on
  selector:
    app: kubia
    // the pod tempelete for creating new pods
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: 844348677/kubia
        ports:
        - containersPort: 8080

when you post the file to the API server, Kubernetes creates a newe ReplicationController named kubia, which nakes sure three pod instances always match teh label selector app=kubia
when there aren't enough pods, new pods will be created from the provided pod template.
not specifying the selector at all is also an option. in that case, it will be configured automatically from the labels in the pod remplate

$ kubectl create -f kubia-rc.yaml

Seeing the ReplicationController in action
because no pods exist with the app=kubia, the ReplicationController should spin up  three new pods from the pod template.

$ kubectl get pods

Seeing the ReplicationController response to a deleted pod
first, you'll delete one of the pods manually to see how the ReplicationController spins up a new immediately, bring the number of matching pods back to three

$ kubectl delete pod kubia-xxx
$ kubectl get pods

Getting information about a ReplicationController

$ kubectl get rc

derised number of pods; the actual number of pods ; and how many of them are ready

$ kubectl describe rc kubia

Understanding exactly what caused the controller to create a new pod
if a pod disappears, the ReplicationController sees too few pods and creates a new replacement pod
while a ReplicationController is immediately notified about a pod being deleted (the API server allows clients to watch for changes to resources and resource lists), that's not what causes it to create replacement pod
the notification triggers the controller to check the actual number of pods and take appropriate action

Responding to a node failure
if a node fails in the non-Kubernetes world, the ops team would need to migrate the applications running on that node to other machines manually
Kubernetes waits a while before resheduling pods (in case the node is unreachable because of a temporary network glitch or because the Kubelet is restarting)
if the node stays unreachable for several minutes, the status of the pods that were sheduled to that node changes to Unkown

Moving pods in and out of the scope of a ReplicationController
a ReplicationController manages pods that match is label selector
by changing a pod's labels, it can be removed from or added to the scope of a ReplicationController

Adding labels to pods managed by a ReplicationControllerr

$ kubectl label pod kubia-dmdck type=special
$ kubectl get pods --show-labels

Changing the labels of a mananged pod

$ kubectl label pod kubia-dmdck app=foo --overwrite
$ kubectl get pods -L app

Removing pods from controllers in practice

Changing teh ReplicationController's label selector
modified the ReplicationController's label selector
kuberntes does allow you to change a ReplicationController's label selector,
you'll never change a controller's label selector, but you'll regularly change its pod template

Changing the pod template
changing a ReplicationController's pod template only affects pod created afterward and has no effect on existing pods

$ kubectl edit rc kubia 

Horizontally scalling pods

Scaling up a ReplicationController

$ kubectl scale rc kubia --replication=10

Scaling a ReplicationController by editing its definition
instead of using the kubectl scale command, you're goint to scale it in a declarative way by editing the ReplicationController's definition

$ kubectl edit rc kubia
$ kubectl get rc

Scaling down with the kubectl scale command

$ kubectl scale rc kubia --replicas=3

All this command does is modify the spec.replicas field of the ReplicationController's definition-like when you changed it through kubectl edit

Understanding the declaration approach to scaling
you're not telling Kubernetes what or how to do it. you're just specifying the desired state

Deleting a ReplicaitonController
through kubectl delete, the pods are also deleted
you can delete only teh ReplicationController and leave the pods  running
this may be useful when you initially have a set of pods managed by a ReplicationController, and then decide to replace the ReplicationController with a ReplicaSet for example
you can do this without affecting the pods and keep them runnign without interruption while you replace the ReplicationController that manages them
you can keep its pods running by passing the --cascade=false option on the command

$ kubectl delete rc kubia --cascade=false

Using ReplicaSets instead of ReplicationControllers
Later , a similar resource called a ReplicaSet was introduced
it's a new generation of ReplicationController and replaces it completely(ReplicationController will eventually be deprecated)
you should always create ReplicaSets instead of ReplicationControllers from now on

comparing a ReplicaSet a ReplicationController
it has more expressive pod selectors
whereas a ReplicationCotnroller's label selector only allows  matching pods that include a certain latel
a ReplicaSet's selector also allows matching pods that lack a  certain label or pods that include a certian label key, regardless of its value
a single ReplicaitonController can't match pods with the label env=production and those with the label env=devel at the same time
but a single ReplicaSet can match both sets of pods and treat them as single group

Defining a ReplicaSet
ch04/kubia-replicaset.yaml

# ReplicaSets aren't part of the v1 API, but belong to the apps API group and version v1beta2
apiVersion: apps/v1beta2
kind: ReplicaSet
metadata:
  name: kubia
spec:
  replicas: 3
  # you're using the simpler matchLabels selector here, which is much like a ReplicationController's selector
  selector:
    matchLabels:
      app: kubia
  # the tamplate is the same as in the ReplicationController
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: 844348677/kubia

the only difference is in the selector. instead of listing labels the pods need to have directly under the selector property, you're specifying them under selector.matchLabels

Creating and examing a ReplicaSet

$ kubectl get rs
$ kubectl describe rs

Using the ReplicaSet's more expressive label selectors
the main improvements of ReplicaSets over ReplicationController are their more expressive label selector
ch04/kubia-replicaset-matchexpressions.yaml

  selector:
    matchExpressions:
      # this selector requires the pod to contain a label with the "app" key
      - key: app
      # the label's value must be "kubia"
      operator: In
      values:
        - kubia

each expression must contain a key, an operator, and possibly (depending on the operator) a list of values
four valid operator
	In - label's value must match one of the specified values
	NotIn - label's value must not match any of the specified values
	Exists - Pod must indluce a label with specified key (the value isn't important). when using this operator, you shouldn't specify the values field
	DoesNotExist - Pod must not include a label with the specified key. the values property must not be specified

if you specify multiple expressions, all those expressions must evaluate to true fro the selector match a pod
if you specify both matchLabels and matchExpressions, all the labels must match and all the expresesions must evaluate to true fro the pod to match the selector

Wrapping up ReplicaSets

$ kubectl delete rs kubia

Running exactly on pod on each node with DaemonSet
when you want a pod to run on each and every node in the cluster ( and each node needs to run exactly on instance of the pod)
Kubernetes'own kube-proxy process, which needs to run on all nodes to make services work
DaemonSet exactly on replica on each node
DaemonSets run only a single pod replica on each node, whereas ReplicaSets scatter them around the whole cluster randomly

Using a DaemonSet to run a pod on every node
to run a pod on all cluster nodes, you create a DaemonSet object
a DaemonSet makes sure it creates as many pods as there are nodes and deploys each one on its own

Using a DaemonSet to run pods only on certain nodes
A DaemonSet deplays pods to all nodes in the cluster, unless you specify that the pods should only run on a subset of all the nodes
this si done by specify the nodeSelecotor property in the pod template
nodes can be made unschedulable, a DaemonSet will  deploy pods even on such nodes, pods managed by a DaemonSet bypass the Scheduler completely

Explaining DaemonSets with an example

Creating a daemonSet YAML definition

you're defining a DaemonSet that will run a pod with a single container based on the luksa/ssd-monitor container image. an instance of this pod will be created for each node that has the disk=ssd label

Creating the DaemonSet

$ kubectl create -f ssd-monitor-daemonset.yaml
$ kubectl get ds
$ kubectl get po

Adding the required label to your node(s)

$ kubectl get node
(NAME minikube)
$ kubectl label node minikube disk=ssd
$ kubectl get po

Removing the required label from the node

$ kubectl label node minikube disk=hdd --overwrite

Running pods that perform a single completely tash
you only want to run a task that terminates after completing its work
ReplicationControllers, ReplicaSet, and DaemonSets run continuous task that are never considered completed. Processes in such pods are restarted when they exit

Introducing the Job resource
Job resource, it allow you to run a pod whose container isn't restarted when then process running inside finishes successfully
the event of a node failure ; the event of a failure of the process itself (when the process returns an error exit code)

Defining a Job resource
ch04/exporter.yaml

Seeing a Job run a pod

$ kubectl get jobs
$ kubectl get po
$ kubectl get po -a
$ kubectl logs batch-job-xxx
$ kubectl get job

Running multiple pod instances in a Job
Jobs many be configured to create more than on pod instance and run them in parallel or sequentially
this is done by setting the completions and the parallelism properties in the Job spec

Running Job pods sequentially
if you need a Job to run more than once, you set completions to how many times you want the Job's pod to run. the following listing shows an example
ch04/multi-completion-batch-job.yaml

this job will run five pods one after the other

Running Job pods in parallel
you can also make the Job run multiple pods in parallel
ch04/multi-compeltion-parallel-batch-job.yaml

$ kubectl get go

Scaling a Job
you can even change a Job's parallelism property while the job running.

$ kubectl scale job multi-completion-batch-job --replicas 3

Limiting the time allowed for a Job pod to complete
a pod's time can be limited by setting the activeDeadlineSeconds property in the pod spec
you can configure how many times a Job can be retried before it is marked as failed by specifying the spec.backoffLimit field in the Job manifest

Scheduling Job to run periodically or once in the future
a cron job in Kubernetes is configured by creating a CronJob resource

Creating a CronJob
ch04/cronjob.yaml

Configuring the schedule
left to right, the schedule contains the following five entries:
	minute
	hour
	day of month
	month
	day  of weed

Understanding how scheduled jobs are run
Job resources will be created from the CronJob resource at approximately the  scheduled time. the Job then creates the pods

# at the latest, the pod must start running at 15 seconds past the scheduled time
startingDeadlineSecond: 15

Summary


ch05 Services: enabling clients to discover and talk to pods
	creating Service resources to expose a group of pods  at a single address
	discovering services in the cluster
	exposing services to external clients
	connecting to external services from inside the cluster
	controlling whether  a pod  is ready to be part of the service or not
	troubleshooting services
in the case of microservices, pods will usually respond to HTTP requests comming either from other pods inside the cluster or from outside the cluster
pods need a way of finding other pods if they want to consume the services they provide
	pods are ephemeral - they may come and go at any time, whether it's because a pod is removed from a node to make room fro other pods, bacause someone scaled down the number of pods, or because a cluster node has failed
	Kubernets assigns an IP address to a pod after the pod has been scheduled to a node and before it's started - client thus can't know the IP address of the server pod up front
	horizontal scaling means multiple pods may provide the same service - each of those pods has its own IP address. clients shouldn't care how may pods are backing the service and what IPs are. they shouldn't have  to keep a list of all the individual IPs of pods. Instead, all those pods should be accessible through a single IP address

Introducing services
A kubernetes Service is a resource you create to make a single, contant point of entry to a group of pods providing the same service
each service has an IP adress and port that never change while the service exists
clients can open connections to that IP and port, and those connections are then routed to one fo the pods backing that service
this way, clients of a service don't need to know the location of individual pods providing the service, allowing  those pods to be moved  around the cluster at any time

Explaing Services with an example
by creating a service for the fronted pods and configuring it to be accessible from outside the cluster, you expose a single, constant IP address through which external clients can connect to the pods
by creating a service for the backend pod, you create a stable address for the backend pod 
the service address doesn't change even if the pod's IP address changes

Creating services
a service can be backed by more than one pod. Connection to the service are load-balanced across all the backing pods
label selectors
Label selectors determine which pods belong to the Service

Creating a service through Kubectl expose
the expose command created a Service resource with the same pod selector as the one used by the RC, thereby exposing all its pods through a single IP address and port
ch05/kubia-svc.yaml

apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  ports:
    # the port this service will be available on
  - port: 80
    # the container port the service will forward to 
    targetPort: 8080
    # all pods with the app=kubia label will be part of this service
  selector:
    app: kubia

Examining your new service

$ kubectl get svc

Testing your service from within the cluster
you can send requests to your service from within the cluster in a few ways
	the obvious way is to create a pod that will send the request to the service's cluster IP and log the request. you can examine the pod's log to see what the service's response was
	you can ssh into one of the Kubernetes nodes and use the curl command
	you can execute the curl command inside on of your existing pods through the kubectl  exec command

Remotely executing commands in running containers
the kubectl exec command allows you to remotely run arbitrary commands inside an existing container of a pod

$ kubectl get pods
$ kubectl get svc
$ kubectl exec kubia-xxx -- curl -s http://xx.xx.xx.xx

everything after the double dash is the command that should be executed inside the pod
when you ran the command
	1. kubectl exec
	2. curl is executed inside the container running node.js
	3. curl sends http get request
	4. service redirects http connection to a randomly selected pod
	5. http response is sent back to curl
	6. the output of the curl command is sent back to kubectl  and printed by it

configuring session affinity on the service
if you want all request made by a certain client to be redirected to the same pod every time, you can set the service's sessionAffinity property to ClientIP (instead of None, which is the default)
Kubernetes supports only two types of service session affinity: None and ClientIP
Kubernetes services don't operate at the HTTP level
cookies  are a construct of the HTTP protocol

Exposing multiple ports in the same service
ch05/kubia-svc-multipleports.yaml

spec:
  # port 80 is mapped to the pods' port 8080
  port:
  - name: http
    port: 80
    targetPort: 8080
    # port 443 is mapped to pods' port 8443
  - name: https
    port: 443
    targetPort: 8443
    # the label selector always applies to the whole  service
  selector:
    app: kubia

Using named ports

Discovering services
Kubernetes also provides ways for client pods to discover a service's IP and port

Discovering services through environment variables
if you create the service before creating the client pods, process in those pods can get the IP address and port of the services by inspecting their environment variables

$ kubectl delete po --all
$ kubectl exec kubia-xxx env

Discovering services through DNS
kube-system  namespece ; one of the pods are called kube-dns
any DNS query performed by a process running in a pod will be handled by Kubernetes'own DNS server, which knows all the services running in you system
each service gets a DNS entry in the internal DNS server, and client pods that know the name of the service can access it through its fully qualified domain name (FQDN) instead of resorting to environment variable

Connecting to the service through its FQDN
backend-database.default.svc.cluster.local
backend-database corresponds to the service name
default stands for the namespace the service is defined in
and svc.cluster.local is configurable cluster domain suffix used in all cluster local service names
you can omit the svc.cluster.local suffix and even the namespace, when the fronted pod is in the same namespace as the database pod

Running a shell in a pod's container
the shell's binary executable must be available in the container image for this to work

$ kubectl exec -it kubia-xxx bash

root@kubia-xxx:/# curl http://kubia.default.svc.cluster.local
root@kubia-xxx:/# curl http://kubia.default
root@kubia-xxx:/# curl http://kubia
root@kubia-xxx:/# cat /etc/resolv.conf

Understanding why you can't ping a service  IP
curling the service works, but pinging it doesn't.
that's because the service's cluster IP is a virtual IP, and only has meaning when combined with the service port

Connecting to services living outside the cluster
instead of having the service redirect connection to pods in the cluster, you want it to redirect to external IP(s) and port(s)
this allows you to take advantage of both service load balancing and service discovery

Introducing service endpoints
Services don't link to pods directly. Instead, a resource sits in between - the Endpioints resource.
an Endpoints resource is a list of IP address an ports exposing a service

$ kubectl get endpoints kubia

the selector is used to build a list of IPs and ports, which is stored in the Endpoints resource.
when a client connects to a service, the service proxy selects one of those IP and port pair and redirects the incoming connection to the server listening at that location

Manually configuring service endpoints
if you create a servic without a pod selector, Kubernetes won't even create the Endpoints resource (after all , without a selector, it can't know which pods to include in the services)
to create a service with manually managed endpoints, you need to create both a Service and an Endpoints resource

Create a service without  a selector
ch05/external-service.yaml

apiVersion: v1
kind: Service
metadata:
  # the name of the service must match the name of the Endpoints object(see next listing)
  name: external-service
# this service has no selector defined
spec:
  ports:
  - port: 80

 Creating an Endpoints resource for a service without a selector
 Endpoints are a separate resource and not an attribute of a service
 ch05/external-service-endpoints.yaml

apiVersion: v1
kind: Endpoints
metadata:
  # the name of the Endpoints object  must match the name of the service (see previous)
  name: external-service
subsets:
  # the IPs of the endpoints that the service will forward connection to
  - addresses:
    - ip: 11.11.11.11
    - ip: 22.22.22.22
    # the target port of the endpoints
    ports:
    - port: 80

the Endpoints object needs to have the same name as the service and contain the list of target IP address and ports for the service
container created after the service is created will include  the environment variables fro the service, and all connections to its IP:port pair will be load balanced between the service's endpoints

Creating an aliea fro an external service
instead fo exposing an external service by manually configuring the service's Endpoints , a simpler method allows you to refer to an external service by its fully qualified domain name (FQDN)

Create an ExternalName service
ch05/external-service-externalname.yaml

apiVersion: v1
kind: Service
metadata:
  name: external-service
spec:
  # service type is set to ExternalNames
  type: ExternalName
  # the fully qualified domain name of the actual service
  externalName: someapi.somecompay.com
  ports:
  - port: 80

after service is created, pods can connect to the external service through the external-service.defualt.svc.cluster.local domain name instead of using the service's actual FQDN (maybe there is IP)
a CNAME record points to a fullly qualified domain name instead of a numeric IP address

Exposing services to external clients
expose certain services, such as fronted webservers, to the outside, so external clients can access them
a few ways to make a service accessible externally
	



